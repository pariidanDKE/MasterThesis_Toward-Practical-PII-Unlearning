{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d0576a0",
   "metadata": {},
   "source": [
    "There is an issue where if the subject is separately tokenized, it is different from ones it is tokenized in the sequence, likely due to the surrounding context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5494fc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No misalignment records found in the log file\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "def parse_misalignment_logs(log_file_path: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Parse tokenization misalignment logs from a log file and convert to DataFrame\n",
    "    \n",
    "    Args:\n",
    "        log_file_path: Path to the log file containing misalignment data\n",
    "        \n",
    "    Returns:\n",
    "        pandas.DataFrame: Parsed misalignment data\n",
    "    \"\"\"\n",
    "    misalignment_records = []\n",
    "    \n",
    "    with open(log_file_path, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            line = line.strip()\n",
    "            \n",
    "            # Look for lines containing MISALIGNMENT_JSON\n",
    "            if \"MISALIGNMENT_JSON:\" in line:\n",
    "                # Extract the JSON part after \"MISALIGNMENT_JSON: \"\n",
    "                json_start = line.find(\"MISALIGNMENT_JSON: \") + len(\"MISALIGNMENT_JSON: \")\n",
    "                json_str = line[json_start:]\n",
    "                \n",
    "                try:\n",
    "                    # Parse the JSON\n",
    "                    data = json.loads(json_str)\n",
    "                    misalignment_records.append(data)\n",
    "                except json.JSONDecodeError as e:\n",
    "                    print(f\"Failed to parse JSON: {e}\")\n",
    "                    print(f\"Problematic line: {line}\")\n",
    "                    continue\n",
    "    \n",
    "    if not misalignment_records:\n",
    "        print(\"No misalignment records found in the log file\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    df = pd.DataFrame(misalignment_records)\n",
    "    \n",
    "    # Add some useful computed columns\n",
    "    df['missing_token_ratio'] = df['missing_token_count'] / df['subject_token_count']\n",
    "    df['has_tokens_before'] = df['actual_tokens_before'].apply(lambda x: len(x) > 0 if x else False)\n",
    "    df['has_tokens_after'] = df['actual_tokens_after'].apply(lambda x: len(x) > 0 if x else False)\n",
    "    df['tokens_before_count'] = df['actual_tokens_before'].apply(lambda x: len(x) if x else 0)\n",
    "    df['tokens_after_count'] = df['actual_tokens_after'].apply(lambda x: len(x) if x else 0)\n",
    "    df['total_replacement_tokens'] = df['tokens_before_count'] + df['tokens_after_count']\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9146569b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject_string</th>\n",
       "      <th>subject_token_ids</th>\n",
       "      <th>subject_tokens_decoded</th>\n",
       "      <th>missing_token_ids</th>\n",
       "      <th>missing_token_strings</th>\n",
       "      <th>full_text_token_ids</th>\n",
       "      <th>full_text_tokens_decoded</th>\n",
       "      <th>full_text_preview</th>\n",
       "      <th>subject_token_count</th>\n",
       "      <th>full_text_token_count</th>\n",
       "      <th>...</th>\n",
       "      <th>actual_tokens_before</th>\n",
       "      <th>actual_tokens_before_decoded</th>\n",
       "      <th>actual_tokens_after</th>\n",
       "      <th>actual_tokens_after_decoded</th>\n",
       "      <th>missing_token_ratio</th>\n",
       "      <th>has_tokens_before</th>\n",
       "      <th>has_tokens_after</th>\n",
       "      <th>tokens_before_count</th>\n",
       "      <th>tokens_after_count</th>\n",
       "      <th>total_replacement_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Einar Vilhelm Svedberg</td>\n",
       "      <td>[36, 14080, 64749, 52999, 328, 2111, 7881]</td>\n",
       "      <td>[E, inar,  Vil, helm,  S, ved, berg]</td>\n",
       "      <td>[36]</td>\n",
       "      <td>[E]</td>\n",
       "      <td>[128000, 128006, 882, 128007, 271, 3923, 574, ...</td>\n",
       "      <td>[&lt;|begin_of_text|&gt;, &lt;|start_header_id|&gt;, user,...</td>\n",
       "      <td>&lt;|start_header_id|&gt;user&lt;|end_header_id|&gt;\\n\\nWh...</td>\n",
       "      <td>7</td>\n",
       "      <td>69</td>\n",
       "      <td>...</td>\n",
       "      <td>[469]</td>\n",
       "      <td>[ E]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Jesper Madsen</td>\n",
       "      <td>[41, 70138, 386, 7819, 268]</td>\n",
       "      <td>[J, esper,  M, ads, en]</td>\n",
       "      <td>[41, 70138]</td>\n",
       "      <td>[J, esper]</td>\n",
       "      <td>[128000, 128006, 882, 128007, 271, 3923, 374, ...</td>\n",
       "      <td>[&lt;|begin_of_text|&gt;, &lt;|start_header_id|&gt;, user,...</td>\n",
       "      <td>&lt;|start_header_id|&gt;user&lt;|end_header_id|&gt;\\n\\nWh...</td>\n",
       "      <td>5</td>\n",
       "      <td>49</td>\n",
       "      <td>...</td>\n",
       "      <td>[9243, 716]</td>\n",
       "      <td>[ Jes, per]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Luzi Albrecht von Moos</td>\n",
       "      <td>[43, 5308, 72, 1708, 21152, 14244, 6675, 6178,...</td>\n",
       "      <td>[L, uz, i,  Al, bre, cht,  von,  Mo, os]</td>\n",
       "      <td>[43, 5308]</td>\n",
       "      <td>[L, uz]</td>\n",
       "      <td>[128000, 128006, 882, 128007, 271, 3923, 574, ...</td>\n",
       "      <td>[&lt;|begin_of_text|&gt;, &lt;|start_header_id|&gt;, user,...</td>\n",
       "      <td>&lt;|start_header_id|&gt;user&lt;|end_header_id|&gt;\\n\\nWh...</td>\n",
       "      <td>9</td>\n",
       "      <td>70</td>\n",
       "      <td>...</td>\n",
       "      <td>[304, 82739]</td>\n",
       "      <td>[ in,  Luz]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Einar Vilhelm Svedberg</td>\n",
       "      <td>[36, 14080, 64749, 52999, 328, 2111, 7881]</td>\n",
       "      <td>[E, inar,  Vil, helm,  S, ved, berg]</td>\n",
       "      <td>[36]</td>\n",
       "      <td>[E]</td>\n",
       "      <td>[128000, 128006, 882, 128007, 271, 3923, 374, ...</td>\n",
       "      <td>[&lt;|begin_of_text|&gt;, &lt;|start_header_id|&gt;, user,...</td>\n",
       "      <td>&lt;|start_header_id|&gt;user&lt;|end_header_id|&gt;\\n\\nWh...</td>\n",
       "      <td>7</td>\n",
       "      <td>48</td>\n",
       "      <td>...</td>\n",
       "      <td>[469]</td>\n",
       "      <td>[ E]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Fernando Llorente Vidal</td>\n",
       "      <td>[37, 944, 4988, 445, 385, 72823, 650, 26966]</td>\n",
       "      <td>[F, ern, ando,  L, lo, rente,  V, idal]</td>\n",
       "      <td>[37, 944, 4988]</td>\n",
       "      <td>[F, ern, ando]</td>\n",
       "      <td>[128000, 128006, 882, 128007, 271, 3923, 374, ...</td>\n",
       "      <td>[&lt;|begin_of_text|&gt;, &lt;|start_header_id|&gt;, user,...</td>\n",
       "      <td>&lt;|start_header_id|&gt;user&lt;|end_header_id|&gt;\\n\\nWh...</td>\n",
       "      <td>8</td>\n",
       "      <td>51</td>\n",
       "      <td>...</td>\n",
       "      <td>[5938, 449, 51485]</td>\n",
       "      <td>[ associated,  with,  Fernando]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Viktor Fedorovich Melnikov</td>\n",
       "      <td>[53, 1609, 11222, 24526, 269, 51214, 11220, 22...</td>\n",
       "      <td>[V, ik, tor,  Fed, or, ovich,  Mel, nik, ov]</td>\n",
       "      <td>[53, 1609, 11222]</td>\n",
       "      <td>[V, ik, tor]</td>\n",
       "      <td>[128000, 128006, 882, 128007, 271, 3923, 374, ...</td>\n",
       "      <td>[&lt;|begin_of_text|&gt;, &lt;|start_header_id|&gt;, user,...</td>\n",
       "      <td>&lt;|start_header_id|&gt;user&lt;|end_header_id|&gt;\\n\\nWh...</td>\n",
       "      <td>9</td>\n",
       "      <td>48</td>\n",
       "      <td>...</td>\n",
       "      <td>[1396, 315, 77116]</td>\n",
       "      <td>[ number,  of,  Viktor]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Luzi Albrecht von Moos</td>\n",
       "      <td>[43, 5308, 72, 1708, 21152, 14244, 6675, 6178,...</td>\n",
       "      <td>[L, uz, i,  Al, bre, cht,  von,  Mo, os]</td>\n",
       "      <td>[43, 5308]</td>\n",
       "      <td>[L, uz]</td>\n",
       "      <td>[128000, 128006, 882, 128007, 271, 3923, 374, ...</td>\n",
       "      <td>[&lt;|begin_of_text|&gt;, &lt;|start_header_id|&gt;, user,...</td>\n",
       "      <td>&lt;|start_header_id|&gt;user&lt;|end_header_id|&gt;\\n\\nWh...</td>\n",
       "      <td>9</td>\n",
       "      <td>51</td>\n",
       "      <td>...</td>\n",
       "      <td>[449, 82739]</td>\n",
       "      <td>[ with,  Luz]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Min-Jae Yoon</td>\n",
       "      <td>[6349, 12278, 6043, 816, 9186]</td>\n",
       "      <td>[Min, -J, ae,  Y, oon]</td>\n",
       "      <td>[6349]</td>\n",
       "      <td>[Min]</td>\n",
       "      <td>[128000, 128006, 882, 128007, 271, 3923, 374, ...</td>\n",
       "      <td>[&lt;|begin_of_text|&gt;, &lt;|start_header_id|&gt;, user,...</td>\n",
       "      <td>&lt;|start_header_id|&gt;user&lt;|end_header_id|&gt;\\n\\nWh...</td>\n",
       "      <td>5</td>\n",
       "      <td>83</td>\n",
       "      <td>...</td>\n",
       "      <td>[3468]</td>\n",
       "      <td>[ Min]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Min-Jae Yoon</td>\n",
       "      <td>[6349, 12278, 6043, 816, 9186]</td>\n",
       "      <td>[Min, -J, ae,  Y, oon]</td>\n",
       "      <td>[6349]</td>\n",
       "      <td>[Min]</td>\n",
       "      <td>[128000, 128006, 882, 128007, 271, 3923, 374, ...</td>\n",
       "      <td>[&lt;|begin_of_text|&gt;, &lt;|start_header_id|&gt;, user,...</td>\n",
       "      <td>&lt;|start_header_id|&gt;user&lt;|end_header_id|&gt;\\n\\nWh...</td>\n",
       "      <td>5</td>\n",
       "      <td>54</td>\n",
       "      <td>...</td>\n",
       "      <td>[3468]</td>\n",
       "      <td>[ Min]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Viktor Fedorovich Melnikov</td>\n",
       "      <td>[53, 1609, 11222, 24526, 269, 51214, 11220, 22...</td>\n",
       "      <td>[V, ik, tor,  Fed, or, ovich,  Mel, nik, ov]</td>\n",
       "      <td>[53, 1609, 11222]</td>\n",
       "      <td>[V, ik, tor]</td>\n",
       "      <td>[128000, 128006, 882, 128007, 271, 3923, 574, ...</td>\n",
       "      <td>[&lt;|begin_of_text|&gt;, &lt;|start_header_id|&gt;, user,...</td>\n",
       "      <td>&lt;|start_header_id|&gt;user&lt;|end_header_id|&gt;\\n\\nWh...</td>\n",
       "      <td>9</td>\n",
       "      <td>67</td>\n",
       "      <td>...</td>\n",
       "      <td>[7901, 304, 77116]</td>\n",
       "      <td>[ transaction,  in,  Viktor]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               subject_string  \\\n",
       "0      Einar Vilhelm Svedberg   \n",
       "1               Jesper Madsen   \n",
       "2      Luzi Albrecht von Moos   \n",
       "3      Einar Vilhelm Svedberg   \n",
       "4     Fernando Llorente Vidal   \n",
       "5  Viktor Fedorovich Melnikov   \n",
       "6      Luzi Albrecht von Moos   \n",
       "7                Min-Jae Yoon   \n",
       "8                Min-Jae Yoon   \n",
       "9  Viktor Fedorovich Melnikov   \n",
       "\n",
       "                                   subject_token_ids  \\\n",
       "0         [36, 14080, 64749, 52999, 328, 2111, 7881]   \n",
       "1                        [41, 70138, 386, 7819, 268]   \n",
       "2  [43, 5308, 72, 1708, 21152, 14244, 6675, 6178,...   \n",
       "3         [36, 14080, 64749, 52999, 328, 2111, 7881]   \n",
       "4       [37, 944, 4988, 445, 385, 72823, 650, 26966]   \n",
       "5  [53, 1609, 11222, 24526, 269, 51214, 11220, 22...   \n",
       "6  [43, 5308, 72, 1708, 21152, 14244, 6675, 6178,...   \n",
       "7                     [6349, 12278, 6043, 816, 9186]   \n",
       "8                     [6349, 12278, 6043, 816, 9186]   \n",
       "9  [53, 1609, 11222, 24526, 269, 51214, 11220, 22...   \n",
       "\n",
       "                         subject_tokens_decoded  missing_token_ids  \\\n",
       "0          [E, inar,  Vil, helm,  S, ved, berg]               [36]   \n",
       "1                       [J, esper,  M, ads, en]        [41, 70138]   \n",
       "2      [L, uz, i,  Al, bre, cht,  von,  Mo, os]         [43, 5308]   \n",
       "3          [E, inar,  Vil, helm,  S, ved, berg]               [36]   \n",
       "4       [F, ern, ando,  L, lo, rente,  V, idal]    [37, 944, 4988]   \n",
       "5  [V, ik, tor,  Fed, or, ovich,  Mel, nik, ov]  [53, 1609, 11222]   \n",
       "6      [L, uz, i,  Al, bre, cht,  von,  Mo, os]         [43, 5308]   \n",
       "7                        [Min, -J, ae,  Y, oon]             [6349]   \n",
       "8                        [Min, -J, ae,  Y, oon]             [6349]   \n",
       "9  [V, ik, tor,  Fed, or, ovich,  Mel, nik, ov]  [53, 1609, 11222]   \n",
       "\n",
       "  missing_token_strings                                full_text_token_ids  \\\n",
       "0                   [E]  [128000, 128006, 882, 128007, 271, 3923, 574, ...   \n",
       "1            [J, esper]  [128000, 128006, 882, 128007, 271, 3923, 374, ...   \n",
       "2               [L, uz]  [128000, 128006, 882, 128007, 271, 3923, 574, ...   \n",
       "3                   [E]  [128000, 128006, 882, 128007, 271, 3923, 374, ...   \n",
       "4        [F, ern, ando]  [128000, 128006, 882, 128007, 271, 3923, 374, ...   \n",
       "5          [V, ik, tor]  [128000, 128006, 882, 128007, 271, 3923, 374, ...   \n",
       "6               [L, uz]  [128000, 128006, 882, 128007, 271, 3923, 374, ...   \n",
       "7                 [Min]  [128000, 128006, 882, 128007, 271, 3923, 374, ...   \n",
       "8                 [Min]  [128000, 128006, 882, 128007, 271, 3923, 374, ...   \n",
       "9          [V, ik, tor]  [128000, 128006, 882, 128007, 271, 3923, 574, ...   \n",
       "\n",
       "                            full_text_tokens_decoded  \\\n",
       "0  [<|begin_of_text|>, <|start_header_id|>, user,...   \n",
       "1  [<|begin_of_text|>, <|start_header_id|>, user,...   \n",
       "2  [<|begin_of_text|>, <|start_header_id|>, user,...   \n",
       "3  [<|begin_of_text|>, <|start_header_id|>, user,...   \n",
       "4  [<|begin_of_text|>, <|start_header_id|>, user,...   \n",
       "5  [<|begin_of_text|>, <|start_header_id|>, user,...   \n",
       "6  [<|begin_of_text|>, <|start_header_id|>, user,...   \n",
       "7  [<|begin_of_text|>, <|start_header_id|>, user,...   \n",
       "8  [<|begin_of_text|>, <|start_header_id|>, user,...   \n",
       "9  [<|begin_of_text|>, <|start_header_id|>, user,...   \n",
       "\n",
       "                                   full_text_preview  subject_token_count  \\\n",
       "0  <|start_header_id|>user<|end_header_id|>\\n\\nWh...                    7   \n",
       "1  <|start_header_id|>user<|end_header_id|>\\n\\nWh...                    5   \n",
       "2  <|start_header_id|>user<|end_header_id|>\\n\\nWh...                    9   \n",
       "3  <|start_header_id|>user<|end_header_id|>\\n\\nWh...                    7   \n",
       "4  <|start_header_id|>user<|end_header_id|>\\n\\nWh...                    8   \n",
       "5  <|start_header_id|>user<|end_header_id|>\\n\\nWh...                    9   \n",
       "6  <|start_header_id|>user<|end_header_id|>\\n\\nWh...                    9   \n",
       "7  <|start_header_id|>user<|end_header_id|>\\n\\nWh...                    5   \n",
       "8  <|start_header_id|>user<|end_header_id|>\\n\\nWh...                    5   \n",
       "9  <|start_header_id|>user<|end_header_id|>\\n\\nWh...                    9   \n",
       "\n",
       "   full_text_token_count  ...  actual_tokens_before  \\\n",
       "0                     69  ...                 [469]   \n",
       "1                     49  ...           [9243, 716]   \n",
       "2                     70  ...          [304, 82739]   \n",
       "3                     48  ...                 [469]   \n",
       "4                     51  ...    [5938, 449, 51485]   \n",
       "5                     48  ...    [1396, 315, 77116]   \n",
       "6                     51  ...          [449, 82739]   \n",
       "7                     83  ...                [3468]   \n",
       "8                     54  ...                [3468]   \n",
       "9                     67  ...    [7901, 304, 77116]   \n",
       "\n",
       "      actual_tokens_before_decoded actual_tokens_after  \\\n",
       "0                             [ E]                  []   \n",
       "1                      [ Jes, per]                  []   \n",
       "2                      [ in,  Luz]                  []   \n",
       "3                             [ E]                  []   \n",
       "4  [ associated,  with,  Fernando]                  []   \n",
       "5          [ number,  of,  Viktor]                  []   \n",
       "6                    [ with,  Luz]                  []   \n",
       "7                           [ Min]                  []   \n",
       "8                           [ Min]                  []   \n",
       "9     [ transaction,  in,  Viktor]                  []   \n",
       "\n",
       "  actual_tokens_after_decoded missing_token_ratio  has_tokens_before  \\\n",
       "0                          []            0.142857               True   \n",
       "1                          []            0.400000               True   \n",
       "2                          []            0.222222               True   \n",
       "3                          []            0.142857               True   \n",
       "4                          []            0.375000               True   \n",
       "5                          []            0.333333               True   \n",
       "6                          []            0.222222               True   \n",
       "7                          []            0.200000               True   \n",
       "8                          []            0.200000               True   \n",
       "9                          []            0.333333               True   \n",
       "\n",
       "   has_tokens_after  tokens_before_count  tokens_after_count  \\\n",
       "0             False                    1                   0   \n",
       "1             False                    2                   0   \n",
       "2             False                    2                   0   \n",
       "3             False                    1                   0   \n",
       "4             False                    3                   0   \n",
       "5             False                    3                   0   \n",
       "6             False                    2                   0   \n",
       "7             False                    1                   0   \n",
       "8             False                    1                   0   \n",
       "9             False                    3                   0   \n",
       "\n",
       "   total_replacement_tokens  \n",
       "0                         1  \n",
       "1                         2  \n",
       "2                         2  \n",
       "3                         1  \n",
       "4                         3  \n",
       "5                         3  \n",
       "6                         2  \n",
       "7                         1  \n",
       "8                         1  \n",
       "9                         3  \n",
       "\n",
       "[10 rows x 21 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab8eddd1",
   "metadata": {},
   "source": [
    "To run the experiment I currently want to run, I need to load the question_answer pairs with the appropriate model_template stuff, create the nice QA pairs (with nice per-model special tokens added as well). Aferwards, I'd need to tokenize subject separately, and the QA separately. And see if everything aligns nicely, if not I need to try it with a space and check then, if not return insightful things on the specific tokenizer and what goes wrong.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f6a368",
   "metadata": {},
   "source": [
    "Investigating the Tokenization Issue - \n",
    "\n",
    "For Llama3's tokenizer, there is an issue where if I tokenizer the subject separately, I get a different sequence of token_ids than I would if the string had any surrounding context (part of a larger string in a QA pair).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fba7c45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NO SPACE:\n",
      "\n",
      "=== Testing: 'hello' ===\n",
      "Llama-2:\n",
      "Tokens: ['▁hello']\n",
      "\n",
      "Llama-3:\n",
      "Tokens: ['hello']\n",
      "Match: True\n",
      "\n",
      "SPACE:\n",
      "\n",
      "=== Testing: 'question\n",
      "\n",
      "hello' ===\n",
      "Llama-2:\n",
      "Tokens: ['▁question', '<0x0A>', '<0x0A>', 'hello']\n",
      "\n",
      "Llama-3:\n",
      "Tokens: ['question', 'ĊĊ', 'hello']\n",
      "Match: True\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load tokenizers\n",
    "tokenizer_llama2 = AutoTokenizer.from_pretrained(\"NousResearch/Llama-2-7b-chat-hf\")\n",
    "tokenizer_llama3 = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3-8B-Instruct\")\n",
    "\n",
    "def tokenizer_prepends_tokens(tokenizer, test_text):\n",
    "    \"\"\"Check if tokenizer prepends space tokens\"\"\"\n",
    "    vocab = tokenizer.get_vocab()\n",
    "    has_space_token = any('Ġ' in token for token in vocab.keys())\n",
    "    \n",
    "    tokens = tokenizer.tokenize(test_text)\n",
    "    print(f\"Tokens: {tokens}\")\n",
    "    \n",
    "    first_token_starts_with_space = len(tokens) > 0 and tokens[0].startswith('Ġ')\n",
    "    return has_space_token and first_token_starts_with_space\n",
    "\n",
    "def check_tokenizer_prepends_tokens(test_text='hello'):\n",
    "    \"\"\"Compare tokenizers for given text\"\"\"\n",
    "    print(f\"\\n=== Testing: '{test_text}' ===\")\n",
    "    \n",
    "    print(\"Llama-2:\")\n",
    "    llama2_prepends = tokenizer_prepends_tokens(tokenizer_llama2, test_text)\n",
    "    \n",
    "    print(\"\\nLlama-3:\")\n",
    "    llama3_prepends = tokenizer_prepends_tokens(tokenizer_llama3, test_text)\n",
    "    \n",
    "    print(f\"Match: {llama2_prepends == llama3_prepends}\")\n",
    "\n",
    "# Run tests\n",
    "print(\"NO SPACE:\")\n",
    "check_tokenizer_prepends_tokens('hello')\n",
    "\n",
    "print(\"\\DOUBLE LINE SPACE:\")\n",
    "check_tokenizer_prepends_tokens('question\\n\\nhello')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6de80d3",
   "metadata": {},
   "source": [
    "We can see that adding a space creates a very different outcome for both models. Specifically, the llama2 tokenizer automatically adds a space characters ( '_' for Llama2) to a string that does not have a space, i.e it prepends the space. This means that when inputting 'Einar Svelberg', it will add extra space itself, while the Llama3 tokenizer will not. So, we can start to understand why having surrounding context makes the tokenization for Llama3 misaligned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f4af9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Without Space in the added to the subject string:\n",
      "🔍 QA PAIR TOKENIZATION COMPARISON\n",
      "================================================================================\n",
      "\n",
      "============================================================\n",
      "Llama-2 Tokenization Analysis\n",
      "============================================================\n",
      "Question tokens (44): ['▁<', '|', 'start', '_', 'header', '_', 'id', '|', '>', 'user', '<', '|', 'end', '_', 'header', '_', 'id', '|', '>', '<0x0A>', '<0x0A>', 'What', '▁is', '▁the', '▁credit', '▁card', '▁number', '▁associated', '▁with', '▁Fernando', '▁L', 'lor', 'ente', '▁Vid', 'al', '?', '<', '|', 'e', 'ot', '_', 'id', '|', '>']\n",
      "Answer tokens (63): ['▁<', '|', 'start', '_', 'header', '_', 'id', '|', '>', 'ass', 'istant', '<', '|', 'end', '_', 'header', '_', 'id', '|', '>', '<0x0A>', '<0x0A>', 'The', '▁credit', '▁card', '▁number', '▁associated', '▁with', '▁Fernando', '▁L', 'lor', 'ente', '▁Vid', 'al', '▁is', '▁', '4', '5', '3', '9', '-', '6', '7', '8', '5', '-', '1', '2', '3', '4', '-', '5', '6', '7', '8', '.<', '|', 'e', 'ot', '_', 'id', '|', '>']\n",
      "Subject tokens (7): ['▁', '▁Fernando', '▁L', 'lor', 'ente', '▁Vid', 'al']\n",
      "Subject token IDs: [29871, 17993, 365, 5095, 2016, 18895, 284]\n",
      "\n",
      "Full sample tokens (107): ['▁<', '|', 'start', '_', 'header', '_', 'id', '|', '>', 'user']...\n",
      "Full sample token IDs (107): [529, 29989, 2962, 29918, 6672, 29918, 333, 29989, 29958, 1792]...\n",
      "\n",
      "Subject token IDs present in full sample: True\n",
      "Subject found at positions: []\n",
      "\n",
      "============================================================\n",
      "Llama-3 Tokenization Analysis\n",
      "============================================================\n",
      "Question tokens (20): ['<|start_header_id|>', 'user', '<|end_header_id|>', 'ĊĊ', 'What', 'Ġis', 'Ġthe', 'Ġcredit', 'Ġcard', 'Ġnumber', 'Ġassociated', 'Ġwith', 'ĠFernando', 'ĠL', 'lo', 'rente', 'ĠV', 'idal', '?', '<|eot_id|>']\n",
      "Answer tokens (31): ['<|start_header_id|>', 'assistant', '<|end_header_id|>', 'ĊĊ', 'The', 'Ġcredit', 'Ġcard', 'Ġnumber', 'Ġassociated', 'Ġwith', 'ĠFernando', 'ĠL', 'lo', 'rente', 'ĠV', 'idal', 'Ġis', 'Ġ', '453', '9', '-', '678', '5', '-', '123', '4', '-', '567', '8', '.', '<|eot_id|>']\n",
      "Subject tokens (6): ['ĠFernando', 'ĠL', 'lo', 'rente', 'ĠV', 'idal']\n",
      "Subject token IDs: [51485, 445, 385, 72823, 650, 26966]\n",
      "\n",
      "Full sample tokens (52): ['<|start_header_id|>', 'user', '<|end_header_id|>', 'ĊĊ', 'What', 'Ġis', 'Ġthe', 'Ġcredit', 'Ġcard', 'Ġnumber']...\n",
      "Full sample token IDs (52): [128006, 882, 128007, 271, 3923, 374, 279, 6807, 3786, 1396]...\n",
      "\n",
      "Subject token IDs present in full sample: True\n",
      "Subject found at positions: [(12, 17), (31, 36)]\n",
      "  Context around position 12-17: ['Ġassociated', 'Ġwith', 'ĠFernando', 'ĠL', 'lo', 'rente', 'ĠV', 'idal', '?', '<|eot_id|>']\n",
      "  Context around position 31-36: ['Ġassociated', 'Ġwith', 'ĠFernando', 'ĠL', 'lo', 'rente', 'ĠV', 'idal', 'Ġis', 'Ġ']\n",
      "\n",
      "============================================================\n",
      "COMPARISON SUMMARY\n",
      "============================================================\n",
      "Subject: ' Fernando Llorente Vidal'\n",
      "Llama-2 subject tokens: ['▁', '▁Fernando', '▁L', 'lor', 'ente', '▁Vid', 'al']\n",
      "Llama-3 subject tokens: ['ĠFernando', 'ĠL', 'lo', 'rente', 'ĠV', 'idal']\n",
      "Same tokenization: False\n",
      "\n",
      "Subject found in full sample:\n",
      "  Llama-2: True at []\n",
      "  Llama-3: True at [(12, 17), (31, 36)]\n",
      "\n",
      "Token count comparison:\n",
      "  Question - Llama-2: 44, Llama-3: 20\n",
      "  Answer - Llama-2: 63, Llama-3: 31\n",
      "  Full - Llama-2: 107, Llama-3: 52\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load tokenizers\n",
    "tokenizer_llama2 = AutoTokenizer.from_pretrained(\"NousResearch/Llama-2-7b-chat-hf\")\n",
    "tokenizer_llama3 = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3-8B-Instruct\")\n",
    "\n",
    "# Sample QA pair\n",
    "question = \"\"\"<|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "What is the credit card number associated with Fernando Llorente Vidal?<|eot_id|>\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "answer = \"\"\"<|start_header_id|>assistant<|end_header_id|>\n",
    "\n",
    "The credit card number associated with Fernando Llorente Vidal is 4539-6785-1234-5678.<|eot_id|>\"\"\"\n",
    "\n",
    "# question = \"\"\"<|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "# Who is the financial consultant currently advising Fernando Llorente Vidal?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "# \"\"\"\n",
    "\n",
    "# answer = \"\"\"\n",
    "# <|start_header_id|>assistant<|end_header_id|>\n",
    "\n",
    "# He is currently being advised by his financial consultant, Sofía Aragón.\"\"\"\n",
    "\n",
    "\n",
    "# Subject to analyze\n",
    "\n",
    "def analyze_qa_tokenization(tokenizer, tokenizer_name, question, answer, subject):\n",
    "    \"\"\"Analyze how QA pair is tokenized and check subject token alignment\"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"{tokenizer_name} Tokenization Analysis\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Tokenize each component\n",
    "    q_tokens = tokenizer.tokenize(question)\n",
    "    a_tokens = tokenizer.tokenize(answer)\n",
    "    subject_tokens = tokenizer.tokenize(subject)\n",
    "    \n",
    "    # Get token IDs\n",
    "    q_token_ids = tokenizer.encode(question, add_special_tokens=False)\n",
    "    a_token_ids = tokenizer.encode(answer, add_special_tokens=False)\n",
    "    subject_token_ids = tokenizer.encode(subject, add_special_tokens=False)\n",
    "    \n",
    "    # Combine full sample\n",
    "    full_sample = f\"{question} {answer}\"\n",
    "    full_tokens = tokenizer.tokenize(full_sample)\n",
    "    full_token_ids = tokenizer.encode(full_sample, add_special_tokens=False)\n",
    "    \n",
    "    print(f\"Question tokens ({len(q_tokens)}): {q_tokens}\")\n",
    "    print(f\"Answer tokens ({len(a_tokens)}): {a_tokens}\")\n",
    "    print(f\"Subject tokens ({len(subject_tokens)}): {subject_tokens}\")\n",
    "    print(f\"Subject token IDs: {subject_token_ids}\")\n",
    "    \n",
    "    print(f\"\\nFull sample tokens ({len(full_tokens)}): {full_tokens[:10]}...\")  # Show first 10\n",
    "    print(f\"Full sample token IDs ({len(full_token_ids)}): {full_token_ids[:10]}...\")  # Show first 10\n",
    "    \n",
    "    # Check if subject token IDs are in full sample\n",
    "    subject_in_full = all(token_id in full_token_ids for token_id in subject_token_ids)\n",
    "    print(f\"\\nSubject token IDs present in full sample: {subject_in_full}\")\n",
    "    \n",
    "    # Find subject positions in full sample\n",
    "    subject_positions = []\n",
    "    for i in range(len(full_token_ids) - len(subject_token_ids) + 1):\n",
    "        if full_token_ids[i:i+len(subject_token_ids)] == subject_token_ids:\n",
    "            subject_positions.append((i, i+len(subject_token_ids)-1))\n",
    "    \n",
    "    print(f\"Subject found at positions: {subject_positions}\")\n",
    "    \n",
    "    # Show tokens around subject occurrences\n",
    "    for start, end in subject_positions:\n",
    "        context_start = max(0, start-2)\n",
    "        context_end = min(len(full_tokens), end+3)\n",
    "        context = full_tokens[context_start:context_end]\n",
    "        print(f\"  Context around position {start}-{end}: {context}\")\n",
    "    \n",
    "    return {\n",
    "        'q_tokens': q_tokens,\n",
    "        'a_tokens': a_tokens,\n",
    "        'subject_tokens': subject_tokens,\n",
    "        'full_tokens': full_tokens,\n",
    "        'subject_token_ids': subject_token_ids,\n",
    "        'full_token_ids': full_token_ids,\n",
    "        'subject_in_full': subject_in_full,\n",
    "        'subject_positions': subject_positions\n",
    "    }\n",
    "\n",
    "def compare_tokenizers():\n",
    "    \"\"\"Compare how both tokenizers handle the QA pair\"\"\"\n",
    "    print(\"🔍 QA PAIR TOKENIZATION COMPARISON\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Analyze with both tokenizers\n",
    "    llama2_results = analyze_qa_tokenization(\n",
    "        tokenizer_llama2, \"Llama-2\", question, answer, subject\n",
    "    )\n",
    "    \n",
    "    llama3_results = analyze_qa_tokenization(\n",
    "        tokenizer_llama3, \"Llama-3\", question, answer, subject\n",
    "    )\n",
    "    \n",
    "    # Compare results\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"COMPARISON SUMMARY\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    print(f\"Subject: '{subject}'\")\n",
    "    print(f\"Llama-2 subject tokens: {llama2_results['subject_tokens']}\")\n",
    "    print(f\"Llama-3 subject tokens: {llama3_results['subject_tokens']}\")\n",
    "    print(f\"Same tokenization: {llama2_results['subject_tokens'] == llama3_results['subject_tokens']}\")\n",
    "    \n",
    "    print(f\"\\nSubject found in full sample:\")\n",
    "    print(f\"  Llama-2: {llama2_results['subject_in_full']} at {llama2_results['subject_positions']}\")\n",
    "    print(f\"  Llama-3: {llama3_results['subject_in_full']} at {llama3_results['subject_positions']}\")\n",
    "    \n",
    "    print(f\"\\nToken count comparison:\")\n",
    "    print(f\"  Question - Llama-2: {len(llama2_results['q_tokens'])}, Llama-3: {len(llama3_results['q_tokens'])}\")\n",
    "    print(f\"  Answer - Llama-2: {len(llama2_results['a_tokens'])}, Llama-3: {len(llama3_results['a_tokens'])}\")\n",
    "    print(f\"  Full - Llama-2: {len(llama2_results['full_tokens'])}, Llama-3: {len(llama3_results['full_tokens'])}\")\n",
    "\n",
    "# Run the analysis\n",
    "print('Without Space in the added to the subject string:')\n",
    "subject = \"Fernando Llorente Vidal\"\n",
    "\n",
    "compare_tokenizers()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ea5f67",
   "metadata": {},
   "source": [
    "Here we confirm what we already know, Llama3 only works with space artificially added (as it does not prepend), while Llama2 does works only with no space artifically added. So, a distinction between the two is definetely required. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b378688",
   "metadata": {},
   "source": [
    "Now will develop a rule that simulates adding context, and then tokenizes the examples and subject with and without the space. It then checks which example actually works (i.e the full subject_ids is present in the full_text_input_id). It does so for "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "d155b33e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Llama-2-7B...\n",
      "✅ Llama-2-7B loaded successfully\n",
      "Loading Llama-3-8B...\n",
      "✅ Llama-3-8B loaded successfully\n",
      "Loading Llama-3.1-8B...\n",
      "✅ Llama-3.1-8B loaded successfully\n",
      "Loading Qwen2.5-7B...\n",
      "✅ Qwen2.5-7B loaded successfully\n",
      "Loading Qwen2.5-14B...\n",
      "✅ Qwen2.5-14B loaded successfully\n",
      "Loading GPT-2...\n",
      "✅ GPT-2 loaded successfully\n",
      "Loading GPT-2-Medium...\n",
      "✅ GPT-2-Medium loaded successfully\n",
      "Loading DeepSeek-V2...\n",
      "✅ DeepSeek-V2 loaded successfully\n",
      "Loading DeepSeek-Coder...\n",
      "✅ DeepSeek-Coder loaded successfully\n",
      "Loading Phi-3...\n",
      "✅ Phi-3 loaded successfully\n",
      "Loading Phi-3.5...\n",
      "✅ Phi-3.5 loaded successfully\n",
      "Loading Gemma-2-2B...\n",
      "✅ Gemma-2-2B loaded successfully\n",
      "Loading Gemma-2-9B...\n",
      "✅ Gemma-2-9B loaded successfully\n",
      "Loading Mistral-7B...\n",
      "✅ Mistral-7B loaded successfully\n",
      "Loading Mistral-Nemo...\n",
      "❌ Failed to load Mistral-Nemo: You are trying to access a gated repo.\n",
      "Make sure to have access to it at https://huggingface.co/mistralai/Mistral-Nemo-Instruct-2407.\n",
      "403 Client Error. (Request ID: Root=1-6841caeb-0831238a012ba322282a55a8;b6e72a9a-c069-4792-b8a3-fbd71ada8177)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/mistralai/Mistral-Nemo-Instruct-2407/resolve/main/config.json.\n",
      "Access to model mistralai/Mistral-Nemo-Instruct-2407 is restricted and you are not in the authorized list. Visit https://huggingface.co/mistralai/Mistral-Nemo-Instruct-2407 to ask for access.\n",
      "Loading CodeLlama-7B...\n",
      "✅ CodeLlama-7B loaded successfully\n",
      "Loading CodeLlama-13B...\n",
      "✅ CodeLlama-13B loaded successfully\n",
      "Loading Falcon-7B...\n",
      "✅ Falcon-7B loaded successfully\n",
      "Loading Vicuna-7B...\n",
      "✅ Vicuna-7B loaded successfully\n",
      "Loading Yi-6B...\n",
      "✅ Yi-6B loaded successfully\n",
      "Loading Zephyr-7B...\n",
      "✅ Zephyr-7B loaded successfully\n",
      "Loading OpenChat-3.5...\n",
      "✅ OpenChat-3.5 loaded successfully\n",
      "Loading Starling-7B...\n",
      "✅ Starling-7B loaded successfully\n",
      "Loading Solar-10.7B...\n",
      "✅ Solar-10.7B loaded successfully\n",
      "\n",
      "⚠️  Failed to load 1 tokenizers:\n",
      "  - Mistral-Nemo: You are trying to access a gated repo.\n",
      "Make sure to have access to it at https://huggingface.co/mistralai/Mistral-Nemo-Instruct-2407.\n",
      "403 Client Error. (Request ID: Root=1-6841caeb-0831238a012ba322282a55a8;b6e72a9a-c069-4792-b8a3-fbd71ada8177)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/mistralai/Mistral-Nemo-Instruct-2407/resolve/main/config.json.\n",
      "Access to model mistralai/Mistral-Nemo-Instruct-2407 is restricted and you are not in the authorized list. Visit https://huggingface.co/mistralai/Mistral-Nemo-Instruct-2407 to ask for access.\n",
      "\n",
      "🎯 Testing with 23 successfully loaded tokenizers\n",
      "\n",
      "🔍 TOKENIZER SPACE RULE DETERMINATION\n",
      "================================================================================\n",
      "\n",
      "==================== Llama-2-7B ====================\n",
      "  Tokenizer Type: SentencePiece\n",
      "  Rule: NO space\n",
      "  Subject used: 'Einar Vilhelm Svedberg'\n",
      "  Tokens (7): ['▁Ein', 'ar', '▁Vil', 'helm', '▁S', 'ved', 'berg']\n",
      "  First few token IDs: [2694, 279, 16450, 9421, 317]...\n",
      "  📋 CHARACTERISTICS:\n",
      "    vocab_size: 32001\n",
      "    space_tokens_count: 1023\n",
      "    space_token_ratio: 51.1%\n",
      "    underscore_prefixes: 1023\n",
      "    g_prefixes: 0\n",
      "    single_space_tokens: ['▁▁']\n",
      "    single_space_ids: [259]\n",
      "    word_boundary_example: ['▁hello']\n",
      "    space_attached_to_next_word: True\n",
      "    tokenizer_class: LlamaTokenizerFast\n",
      "    model_path: Llama-2-7b-chat-hf\n",
      "    unk_token: <unk>\n",
      "    unusual_char_handling: ['▁c', 'afé']\n",
      "    has_byte_level_tokens: False\n",
      "  Validation: ✅ WORKS\n",
      "  Found at position: 3\n",
      "\n",
      "==================== Llama-3-8B ====================\n",
      "  Tokenizer Type: BPE\n",
      "  Rule: ADD space\n",
      "  Subject used: ' Einar Vilhelm Svedberg'\n",
      "  Tokens (7): ['ĠE', 'inar', 'ĠVil', 'helm', 'ĠS', 'ved', 'berg']\n",
      "  First few token IDs: [469, 14080, 64749, 52999, 328]...\n",
      "  📋 CHARACTERISTICS:\n",
      "    vocab_size: 128256\n",
      "    space_tokens_count: 909\n",
      "    space_token_ratio: 45.5%\n",
      "    underscore_prefixes: 0\n",
      "    g_prefixes: 905\n",
      "    single_space_tokens: ['Ġ']\n",
      "    single_space_ids: [220]\n",
      "    word_boundary_example: ['hello']\n",
      "    space_attached_to_next_word: False\n",
      "    tokenizer_class: PreTrainedTokenizerFast\n",
      "    model_path: Meta-Llama-3-8B-Instruct\n",
      "    unusual_char_handling: ['ca', 'fÃ©']\n",
      "    has_byte_level_tokens: False\n",
      "  Validation: ✅ WORKS\n",
      "  Found at position: 3\n",
      "\n",
      "==================== Llama-3.1-8B ====================\n",
      "  Tokenizer Type: BPE\n",
      "  Rule: ADD space\n",
      "  Subject used: ' Einar Vilhelm Svedberg'\n",
      "  Tokens (7): ['ĠE', 'inar', 'ĠVil', 'helm', 'ĠS', 'ved', 'berg']\n",
      "  First few token IDs: [469, 14080, 64749, 52999, 328]...\n",
      "  📋 CHARACTERISTICS:\n",
      "    vocab_size: 128256\n",
      "    space_tokens_count: 904\n",
      "    space_token_ratio: 45.2%\n",
      "    underscore_prefixes: 0\n",
      "    g_prefixes: 902\n",
      "    single_space_tokens: ['Ġ']\n",
      "    single_space_ids: [220]\n",
      "    word_boundary_example: ['hello']\n",
      "    space_attached_to_next_word: False\n",
      "    tokenizer_class: PreTrainedTokenizerFast\n",
      "    model_path: Meta-Llama-3.1-8B-Instruct\n",
      "    unusual_char_handling: ['ca', 'fÃ©']\n",
      "    has_byte_level_tokens: False\n",
      "  Validation: ✅ WORKS\n",
      "  Found at position: 3\n",
      "\n",
      "==================== Qwen2.5-7B ====================\n",
      "  Tokenizer Type: BPE\n",
      "  Rule: ADD space\n",
      "  Subject used: ' Einar Vilhelm Svedberg'\n",
      "  Tokens (7): ['ĠE', 'inar', 'ĠVil', 'helm', 'ĠS', 'ved', 'berg']\n",
      "  First few token IDs: [468, 13762, 63649, 51899, 328]...\n",
      "  📋 CHARACTERISTICS:\n",
      "    vocab_size: 151665\n",
      "    space_tokens_count: 693\n",
      "    space_token_ratio: 34.6%\n",
      "    underscore_prefixes: 0\n",
      "    g_prefixes: 692\n",
      "    single_space_tokens: ['Ġ']\n",
      "    single_space_ids: [220]\n",
      "    word_boundary_example: ['hello']\n",
      "    space_attached_to_next_word: False\n",
      "    tokenizer_class: Qwen2TokenizerFast\n",
      "    model_path: Qwen2.5-7B-Instruct\n",
      "    unusual_char_handling: ['ca', 'fÃ©']\n",
      "    has_byte_level_tokens: False\n",
      "  Validation: ✅ WORKS\n",
      "  Found at position: 3\n",
      "\n",
      "==================== Qwen2.5-14B ====================\n",
      "  Tokenizer Type: BPE\n",
      "  Rule: ADD space\n",
      "  Subject used: ' Einar Vilhelm Svedberg'\n",
      "  Tokens (7): ['ĠE', 'inar', 'ĠVil', 'helm', 'ĠS', 'ved', 'berg']\n",
      "  First few token IDs: [468, 13762, 63649, 51899, 328]...\n",
      "  📋 CHARACTERISTICS:\n",
      "    vocab_size: 151665\n",
      "    space_tokens_count: 742\n",
      "    space_token_ratio: 37.1%\n",
      "    underscore_prefixes: 0\n",
      "    g_prefixes: 739\n",
      "    single_space_tokens: ['Ġ']\n",
      "    single_space_ids: [220]\n",
      "    word_boundary_example: ['hello']\n",
      "    space_attached_to_next_word: False\n",
      "    tokenizer_class: Qwen2TokenizerFast\n",
      "    model_path: Qwen2.5-14B-Instruct\n",
      "    unusual_char_handling: ['ca', 'fÃ©']\n",
      "    has_byte_level_tokens: False\n",
      "  Validation: ✅ WORKS\n",
      "  Found at position: 3\n",
      "\n",
      "==================== GPT-2 ====================\n",
      "  Tokenizer Type: BPE\n",
      "  Rule: ADD space\n",
      "  Subject used: ' Einar Vilhelm Svedberg'\n",
      "  Tokens (7): ['ĠE', 'inar', 'ĠVil', 'helm', 'ĠS', 'ved', 'berg']\n",
      "  First few token IDs: [412, 22050, 34037, 33485, 311]...\n",
      "  📋 CHARACTERISTICS:\n",
      "    vocab_size: 50257\n",
      "    space_tokens_count: 1298\n",
      "    space_token_ratio: 64.9%\n",
      "    underscore_prefixes: 0\n",
      "    g_prefixes: 1298\n",
      "    single_space_tokens: ['Ġ']\n",
      "    single_space_ids: [220]\n",
      "    word_boundary_example: ['hello']\n",
      "    space_attached_to_next_word: False\n",
      "    tokenizer_class: GPT2TokenizerFast\n",
      "    model_path: gpt2\n",
      "    unk_token: <|endoftext|>\n",
      "    unusual_char_handling: ['c', 'af', 'Ã©']\n",
      "    has_byte_level_tokens: False\n",
      "  Validation: ✅ WORKS\n",
      "  Found at position: 4\n",
      "\n",
      "==================== GPT-2-Medium ====================\n",
      "  Tokenizer Type: BPE\n",
      "  Rule: ADD space\n",
      "  Subject used: ' Einar Vilhelm Svedberg'\n",
      "  Tokens (7): ['ĠE', 'inar', 'ĠVil', 'helm', 'ĠS', 'ved', 'berg']\n",
      "  First few token IDs: [412, 22050, 34037, 33485, 311]...\n",
      "  📋 CHARACTERISTICS:\n",
      "    vocab_size: 50257\n",
      "    space_tokens_count: 1301\n",
      "    space_token_ratio: 65.0%\n",
      "    underscore_prefixes: 0\n",
      "    g_prefixes: 1301\n",
      "    single_space_tokens: ['Ġ']\n",
      "    single_space_ids: [220]\n",
      "    word_boundary_example: ['hello']\n",
      "    space_attached_to_next_word: False\n",
      "    tokenizer_class: GPT2TokenizerFast\n",
      "    model_path: gpt2-medium\n",
      "    unk_token: <|endoftext|>\n",
      "    unusual_char_handling: ['c', 'af', 'Ã©']\n",
      "    has_byte_level_tokens: False\n",
      "  Validation: ✅ WORKS\n",
      "  Found at position: 4\n",
      "\n",
      "==================== DeepSeek-V2 ====================\n",
      "  Tokenizer Type: BPE\n",
      "  Rule: ADD space\n",
      "  Subject used: ' Einar Vilhelm Svedberg'\n",
      "  Tokens (7): ['ĠE', 'inar', 'ĠVil', 'helm', 'ĠS', 'ved', 'berg']\n",
      "  First few token IDs: [427, 14523, 19790, 30428, 324]...\n",
      "  📋 CHARACTERISTICS:\n",
      "    vocab_size: 100002\n",
      "    space_tokens_count: 916\n",
      "    space_token_ratio: 45.8%\n",
      "    underscore_prefixes: 0\n",
      "    g_prefixes: 915\n",
      "    single_space_tokens: ['Ġ']\n",
      "    single_space_ids: [207]\n",
      "    word_boundary_example: ['hello']\n",
      "    space_attached_to_next_word: False\n",
      "    tokenizer_class: LlamaTokenizerFast\n",
      "    model_path: DeepSeek-V2-Lite-Chat\n",
      "    unusual_char_handling: ['caf', 'Ã©']\n",
      "    has_byte_level_tokens: False\n",
      "  Validation: ✅ WORKS\n",
      "  Found at position: 3\n",
      "\n",
      "==================== DeepSeek-Coder ====================\n",
      "  Tokenizer Type: BPE\n",
      "  Rule: ADD space\n",
      "  Subject used: ' Einar Vilhelm Svedberg'\n",
      "  Tokens (7): ['ĠE', 'inar', 'ĠVil', 'helm', 'ĠS', 'ved', 'berg']\n",
      "  First few token IDs: [426, 14566, 19869, 30576, 324]...\n",
      "  📋 CHARACTERISTICS:\n",
      "    vocab_size: 32022\n",
      "    space_tokens_count: 945\n",
      "    space_token_ratio: 47.2%\n",
      "    underscore_prefixes: 0\n",
      "    g_prefixes: 944\n",
      "    single_space_tokens: ['Ġ']\n",
      "    single_space_ids: [207]\n",
      "    word_boundary_example: ['hello']\n",
      "    space_attached_to_next_word: False\n",
      "    tokenizer_class: LlamaTokenizerFast\n",
      "    model_path: deepseek-coder-6.7b-instruct\n",
      "    unusual_char_handling: ['c', 'af', 'Ã©']\n",
      "    has_byte_level_tokens: False\n",
      "  Validation: ✅ WORKS\n",
      "  Found at position: 5\n",
      "\n",
      "==================== Phi-3 ====================\n",
      "  Tokenizer Type: SentencePiece\n",
      "  Rule: NO space\n",
      "  Subject used: 'Einar Vilhelm Svedberg'\n",
      "  Tokens (7): ['▁Ein', 'ar', '▁Vil', 'helm', '▁S', 'ved', 'berg']\n",
      "  First few token IDs: [2694, 279, 16450, 9421, 317]...\n",
      "  📋 CHARACTERISTICS:\n",
      "    vocab_size: 32011\n",
      "    space_tokens_count: 1008\n",
      "    space_token_ratio: 50.4%\n",
      "    underscore_prefixes: 1008\n",
      "    g_prefixes: 0\n",
      "    single_space_tokens: ['▁▁']\n",
      "    single_space_ids: [259]\n",
      "    word_boundary_example: ['▁hello']\n",
      "    space_attached_to_next_word: True\n",
      "    tokenizer_class: LlamaTokenizerFast\n",
      "    model_path: Phi-3-mini-4k-instruct\n",
      "    unk_token: <unk>\n",
      "    unusual_char_handling: ['▁c', 'afé']\n",
      "    has_byte_level_tokens: False\n",
      "  Validation: ✅ WORKS\n",
      "  Found at position: 3\n",
      "\n",
      "==================== Phi-3.5 ====================\n",
      "  Tokenizer Type: SentencePiece\n",
      "  Rule: NO space\n",
      "  Subject used: 'Einar Vilhelm Svedberg'\n",
      "  Tokens (7): ['▁Ein', 'ar', '▁Vil', 'helm', '▁S', 'ved', 'berg']\n",
      "  First few token IDs: [2694, 279, 16450, 9421, 317]...\n",
      "  📋 CHARACTERISTICS:\n",
      "    vocab_size: 32011\n",
      "    space_tokens_count: 1042\n",
      "    space_token_ratio: 52.1%\n",
      "    underscore_prefixes: 1042\n",
      "    g_prefixes: 0\n",
      "    single_space_tokens: ['▁▁']\n",
      "    single_space_ids: [259]\n",
      "    word_boundary_example: ['▁hello']\n",
      "    space_attached_to_next_word: True\n",
      "    tokenizer_class: LlamaTokenizerFast\n",
      "    model_path: Phi-3.5-mini-instruct\n",
      "    unk_token: <unk>\n",
      "    unusual_char_handling: ['▁c', 'afé']\n",
      "    has_byte_level_tokens: False\n",
      "  Validation: ✅ WORKS\n",
      "  Found at position: 3\n",
      "\n",
      "==================== Gemma-2-2B ====================\n",
      "  Tokenizer Type: SentencePiece\n",
      "  Rule: ADD space\n",
      "  Subject used: ' Einar Vilhelm Svedberg'\n",
      "  Tokens (7): ['▁Ein', 'ar', '▁Vil', 'helm', '▁S', 'ved', 'berg']\n",
      "  First few token IDs: [8683, 486, 35428, 29421, 570]...\n",
      "  📋 CHARACTERISTICS:\n",
      "    vocab_size: 256000\n",
      "    space_tokens_count: 999\n",
      "    space_token_ratio: 50.0%\n",
      "    underscore_prefixes: 999\n",
      "    g_prefixes: 0\n",
      "    single_space_tokens: ['▁']\n",
      "    single_space_ids: [235248]\n",
      "    word_boundary_example: ['hello']\n",
      "    space_attached_to_next_word: False\n",
      "    tokenizer_class: GemmaTokenizerFast\n",
      "    model_path: gemma-2-2b-it\n",
      "    unk_token: <unk>\n",
      "    unusual_char_handling: ['café']\n",
      "    has_byte_level_tokens: False\n",
      "  Validation: ✅ WORKS\n",
      "  Found at position: 2\n",
      "\n",
      "==================== Gemma-2-9B ====================\n",
      "  Tokenizer Type: SentencePiece\n",
      "  Rule: ADD space\n",
      "  Subject used: ' Einar Vilhelm Svedberg'\n",
      "  Tokens (7): ['▁Ein', 'ar', '▁Vil', 'helm', '▁S', 'ved', 'berg']\n",
      "  First few token IDs: [8683, 486, 35428, 29421, 570]...\n",
      "  📋 CHARACTERISTICS:\n",
      "    vocab_size: 256000\n",
      "    space_tokens_count: 930\n",
      "    space_token_ratio: 46.5%\n",
      "    underscore_prefixes: 930\n",
      "    g_prefixes: 0\n",
      "    single_space_tokens: ['▁']\n",
      "    single_space_ids: [235248]\n",
      "    word_boundary_example: ['hello']\n",
      "    space_attached_to_next_word: False\n",
      "    tokenizer_class: GemmaTokenizerFast\n",
      "    model_path: gemma-2-9b-it\n",
      "    unk_token: <unk>\n",
      "    unusual_char_handling: ['café']\n",
      "    has_byte_level_tokens: False\n",
      "  Validation: ✅ WORKS\n",
      "  Found at position: 2\n",
      "\n",
      "==================== Mistral-7B ====================\n",
      "  Tokenizer Type: SentencePiece\n",
      "  Rule: NO space\n",
      "  Subject used: 'Einar Vilhelm Svedberg'\n",
      "  Tokens (7): ['▁Ein', 'ar', '▁Vil', 'helm', '▁S', 'ved', 'berg']\n",
      "  First few token IDs: [6772, 1051, 24183, 16748, 1086]...\n",
      "  📋 CHARACTERISTICS:\n",
      "    vocab_size: 32768\n",
      "    space_tokens_count: 965\n",
      "    space_token_ratio: 48.2%\n",
      "    underscore_prefixes: 965\n",
      "    g_prefixes: 0\n",
      "    single_space_tokens: ['▁']\n",
      "    single_space_ids: [29473]\n",
      "    word_boundary_example: ['▁hell', 'o']\n",
      "    space_attached_to_next_word: True\n",
      "    tokenizer_class: LlamaTokenizerFast\n",
      "    model_path: Mistral-7B-Instruct-v0.3\n",
      "    unk_token: <unk>\n",
      "    unusual_char_handling: ['▁café']\n",
      "    has_byte_level_tokens: False\n",
      "  Validation: ✅ WORKS\n",
      "  Found at position: 2\n",
      "\n",
      "==================== CodeLlama-7B ====================\n",
      "  Tokenizer Type: SentencePiece\n",
      "  Rule: NO space\n",
      "  Subject used: 'Einar Vilhelm Svedberg'\n",
      "  Tokens (7): ['▁Ein', 'ar', '▁Vil', 'helm', '▁S', 'ved', 'berg']\n",
      "  First few token IDs: [2694, 279, 16450, 9421, 317]...\n",
      "  📋 CHARACTERISTICS:\n",
      "    vocab_size: 32016\n",
      "    space_tokens_count: 1007\n",
      "    space_token_ratio: 50.3%\n",
      "    underscore_prefixes: 1007\n",
      "    g_prefixes: 0\n",
      "    single_space_tokens: ['▁▁']\n",
      "    single_space_ids: [259]\n",
      "    word_boundary_example: ['▁hello']\n",
      "    space_attached_to_next_word: True\n",
      "    tokenizer_class: CodeLlamaTokenizerFast\n",
      "    model_path: CodeLlama-7b-Instruct-hf\n",
      "    unk_token: <unk>\n",
      "    unusual_char_handling: ['▁c', 'afé']\n",
      "    has_byte_level_tokens: False\n",
      "  Validation: ✅ WORKS\n",
      "  Found at position: 3\n",
      "\n",
      "==================== CodeLlama-13B ====================\n",
      "  Tokenizer Type: SentencePiece\n",
      "  Rule: NO space\n",
      "  Subject used: 'Einar Vilhelm Svedberg'\n",
      "  Tokens (7): ['▁Ein', 'ar', '▁Vil', 'helm', '▁S', 'ved', 'berg']\n",
      "  First few token IDs: [2694, 279, 16450, 9421, 317]...\n",
      "  📋 CHARACTERISTICS:\n",
      "    vocab_size: 32016\n",
      "    space_tokens_count: 1024\n",
      "    space_token_ratio: 51.2%\n",
      "    underscore_prefixes: 1024\n",
      "    g_prefixes: 0\n",
      "    single_space_tokens: ['▁▁']\n",
      "    single_space_ids: [259]\n",
      "    word_boundary_example: ['▁hello']\n",
      "    space_attached_to_next_word: True\n",
      "    tokenizer_class: CodeLlamaTokenizerFast\n",
      "    model_path: CodeLlama-13b-Instruct-hf\n",
      "    unk_token: <unk>\n",
      "    unusual_char_handling: ['▁c', 'afé']\n",
      "    has_byte_level_tokens: False\n",
      "  Validation: ✅ WORKS\n",
      "  Found at position: 3\n",
      "\n",
      "==================== Falcon-7B ====================\n",
      "  Tokenizer Type: BPE\n",
      "  Rule: ADD space\n",
      "  Subject used: ' Einar Vilhelm Svedberg'\n",
      "  Tokens (7): ['ĠE', 'inar', 'ĠVil', 'helm', 'ĠS', 'ved', 'berg']\n",
      "  First few token IDs: [399, 5871, 35797, 34877, 302]...\n",
      "  📋 CHARACTERISTICS:\n",
      "    vocab_size: 65024\n",
      "    space_tokens_count: 1225\n",
      "    space_token_ratio: 61.3%\n",
      "    underscore_prefixes: 0\n",
      "    g_prefixes: 1223\n",
      "    single_space_tokens: ['Ġ']\n",
      "    single_space_ids: [204]\n",
      "    word_boundary_example: ['hello']\n",
      "    space_attached_to_next_word: False\n",
      "    tokenizer_class: PreTrainedTokenizerFast\n",
      "    model_path: falcon-7b-instruct\n",
      "    unusual_char_handling: ['c', 'af', 'Ã©']\n",
      "    has_byte_level_tokens: False\n",
      "  Validation: ✅ WORKS\n",
      "  Found at position: 4\n",
      "\n",
      "==================== Vicuna-7B ====================\n",
      "  Tokenizer Type: SentencePiece\n",
      "  Rule: NO space\n",
      "  Subject used: 'Einar Vilhelm Svedberg'\n",
      "  Tokens (7): ['▁Ein', 'ar', '▁Vil', 'helm', '▁S', 'ved', 'berg']\n",
      "  First few token IDs: [2694, 279, 16450, 9421, 317]...\n",
      "  📋 CHARACTERISTICS:\n",
      "    vocab_size: 32000\n",
      "    space_tokens_count: 1033\n",
      "    space_token_ratio: 51.6%\n",
      "    underscore_prefixes: 1033\n",
      "    g_prefixes: 0\n",
      "    single_space_tokens: ['▁']\n",
      "    single_space_ids: [29871]\n",
      "    word_boundary_example: ['▁hello']\n",
      "    space_attached_to_next_word: True\n",
      "    tokenizer_class: LlamaTokenizerFast\n",
      "    model_path: vicuna-7b-v1.5\n",
      "    unk_token: <unk>\n",
      "    unusual_char_handling: ['▁c', 'afé']\n",
      "    has_byte_level_tokens: False\n",
      "  Validation: ✅ WORKS\n",
      "  Found at position: 3\n",
      "\n",
      "==================== Yi-6B ====================\n",
      "  Tokenizer Type: SentencePiece\n",
      "  Rule: ADD space\n",
      "  Subject used: ' Einar Vilhelm Svedberg'\n",
      "  Tokens (8): ['▁E', 'inar', '▁V', 'il', 'helm', '▁S', 'ved', 'berg']\n",
      "  First few token IDs: [764, 17370, 1008, 604, 57504]...\n",
      "  📋 CHARACTERISTICS:\n",
      "    vocab_size: 63992\n",
      "    space_tokens_count: 746\n",
      "    space_token_ratio: 37.3%\n",
      "    underscore_prefixes: 746\n",
      "    g_prefixes: 0\n",
      "    single_space_tokens: ['▁']\n",
      "    single_space_ids: [59568]\n",
      "    word_boundary_example: ['hello']\n",
      "    space_attached_to_next_word: False\n",
      "    tokenizer_class: LlamaTokenizerFast\n",
      "    model_path: Yi-6B-Chat\n",
      "    unk_token: <unk>\n",
      "    unusual_char_handling: ['c', 'af', 'é']\n",
      "    has_byte_level_tokens: True\n",
      "  Validation: ✅ WORKS\n",
      "  Found at position: 3\n",
      "\n",
      "==================== Zephyr-7B ====================\n",
      "  Tokenizer Type: SentencePiece\n",
      "  Rule: NO space\n",
      "  Subject used: 'Einar Vilhelm Svedberg'\n",
      "  Tokens (7): ['▁Ein', 'ar', '▁Vil', 'helm', '▁S', 'ved', 'berg']\n",
      "  First few token IDs: [6004, 283, 23415, 15980, 318]...\n",
      "  📋 CHARACTERISTICS:\n",
      "    vocab_size: 32000\n",
      "    space_tokens_count: 986\n",
      "    space_token_ratio: 49.3%\n",
      "    underscore_prefixes: 986\n",
      "    g_prefixes: 0\n",
      "    single_space_tokens: ['▁▁']\n",
      "    single_space_ids: [259]\n",
      "    word_boundary_example: ['▁hell', 'o']\n",
      "    space_attached_to_next_word: True\n",
      "    tokenizer_class: LlamaTokenizerFast\n",
      "    model_path: zephyr-7b-beta\n",
      "    unk_token: <unk>\n",
      "    unusual_char_handling: ['▁café']\n",
      "    has_byte_level_tokens: False\n",
      "  Validation: ✅ WORKS\n",
      "  Found at position: 2\n",
      "\n",
      "==================== OpenChat-3.5 ====================\n",
      "  Tokenizer Type: SentencePiece\n",
      "  Rule: NO space\n",
      "  Subject used: 'Einar Vilhelm Svedberg'\n",
      "  Tokens (7): ['▁Ein', 'ar', '▁Vil', 'helm', '▁S', 'ved', 'berg']\n",
      "  First few token IDs: [6004, 283, 23415, 15980, 318]...\n",
      "  📋 CHARACTERISTICS:\n",
      "    vocab_size: 32002\n",
      "    space_tokens_count: 1012\n",
      "    space_token_ratio: 50.6%\n",
      "    underscore_prefixes: 1012\n",
      "    g_prefixes: 0\n",
      "    single_space_tokens: ['▁▁']\n",
      "    single_space_ids: [259]\n",
      "    word_boundary_example: ['▁hell', 'o']\n",
      "    space_attached_to_next_word: True\n",
      "    tokenizer_class: LlamaTokenizerFast\n",
      "    model_path: openchat-3.5-0106\n",
      "    unk_token: <unk>\n",
      "    unusual_char_handling: ['▁café']\n",
      "    has_byte_level_tokens: False\n",
      "  Validation: ✅ WORKS\n",
      "  Found at position: 2\n",
      "\n",
      "==================== Starling-7B ====================\n",
      "  Tokenizer Type: SentencePiece\n",
      "  Rule: NO space\n",
      "  Subject used: 'Einar Vilhelm Svedberg'\n",
      "  Tokens (7): ['▁Ein', 'ar', '▁Vil', 'helm', '▁S', 'ved', 'berg']\n",
      "  First few token IDs: [6004, 283, 23415, 15980, 318]...\n",
      "  📋 CHARACTERISTICS:\n",
      "    vocab_size: 32002\n",
      "    space_tokens_count: 1015\n",
      "    space_token_ratio: 50.7%\n",
      "    underscore_prefixes: 1015\n",
      "    g_prefixes: 0\n",
      "    single_space_tokens: ['▁▁']\n",
      "    single_space_ids: [259]\n",
      "    word_boundary_example: ['▁hell', 'o']\n",
      "    space_attached_to_next_word: True\n",
      "    tokenizer_class: LlamaTokenizerFast\n",
      "    model_path: Starling-LM-7B-beta\n",
      "    unk_token: <unk>\n",
      "    unusual_char_handling: ['▁café']\n",
      "    has_byte_level_tokens: False\n",
      "  Validation: ✅ WORKS\n",
      "  Found at position: 2\n",
      "\n",
      "==================== Solar-10.7B ====================\n",
      "  Tokenizer Type: SentencePiece\n",
      "  Rule: NO space\n",
      "  Subject used: 'Einar Vilhelm Svedberg'\n",
      "  Tokens (7): ['▁Ein', 'ar', '▁Vil', 'helm', '▁S', 'ved', 'berg']\n",
      "  First few token IDs: [6004, 283, 23415, 15980, 318]...\n",
      "  📋 CHARACTERISTICS:\n",
      "    vocab_size: 32000\n",
      "    space_tokens_count: 974\n",
      "    space_token_ratio: 48.7%\n",
      "    underscore_prefixes: 974\n",
      "    g_prefixes: 0\n",
      "    single_space_tokens: ['▁▁']\n",
      "    single_space_ids: [259]\n",
      "    word_boundary_example: ['▁hell', 'o']\n",
      "    space_attached_to_next_word: True\n",
      "    tokenizer_class: LlamaTokenizerFast\n",
      "    model_path: SOLAR-10.7B-Instruct-v1.0\n",
      "    unk_token: <unk>\n",
      "    unusual_char_handling: ['▁café']\n",
      "    has_byte_level_tokens: False\n",
      "  Validation: ✅ WORKS\n",
      "  Found at position: 2\n",
      "\n",
      "🎯 COMPREHENSIVE SUMMARY\n",
      "================================================================================\n",
      "Model                Type            Rule            Status    \n",
      "--------------------------------------------------------------------------------\n",
      "Llama-2-7B           SentencePiece   NO space        ✅         \n",
      "Llama-3-8B           BPE             ADD space       ✅         \n",
      "Llama-3.1-8B         BPE             ADD space       ✅         \n",
      "Qwen2.5-7B           BPE             ADD space       ✅         \n",
      "Qwen2.5-14B          BPE             ADD space       ✅         \n",
      "GPT-2                BPE             ADD space       ✅         \n",
      "GPT-2-Medium         BPE             ADD space       ✅         \n",
      "DeepSeek-V2          BPE             ADD space       ✅         \n",
      "DeepSeek-Coder       BPE             ADD space       ✅         \n",
      "Phi-3                SentencePiece   NO space        ✅         \n",
      "Phi-3.5              SentencePiece   NO space        ✅         \n",
      "Gemma-2-2B           SentencePiece   ADD space       ✅         \n",
      "Gemma-2-9B           SentencePiece   ADD space       ✅         \n",
      "Mistral-7B           SentencePiece   NO space        ✅         \n",
      "CodeLlama-7B         SentencePiece   NO space        ✅         \n",
      "CodeLlama-13B        SentencePiece   NO space        ✅         \n",
      "Falcon-7B            BPE             ADD space       ✅         \n",
      "Vicuna-7B            SentencePiece   NO space        ✅         \n",
      "Yi-6B                SentencePiece   ADD space       ✅         \n",
      "Zephyr-7B            SentencePiece   NO space        ✅         \n",
      "OpenChat-3.5         SentencePiece   NO space        ✅         \n",
      "Starling-7B          SentencePiece   NO space        ✅         \n",
      "Solar-10.7B          SentencePiece   NO space        ✅         \n",
      "\n",
      "📊 ANALYSIS BY TOKENIZER TYPE:\n",
      "------------------------------------------------------------\n",
      "\n",
      "SentencePiece (14 models):\n",
      "  - ADD space: 3 models\n",
      "  - NO space: 11 models\n",
      "  ⚠️  Mixed behavior within type\n",
      "\n",
      "BPE (9 models):\n",
      "  - ADD space: 9 models\n",
      "  - NO space: 0 models\n",
      "  🎯 Consistent behavior: ADD space\n",
      "\n",
      "📈 SUCCESS RATE: 23/23 (100.0%)\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "def find_subsequence(haystack, needle):\n",
    "    \"\"\"Find if needle subsequence exists in haystack\"\"\"\n",
    "    for i in range(len(haystack) - len(needle) + 1):\n",
    "        if haystack[i:i+len(needle)] == needle:\n",
    "            return i\n",
    "    return -1\n",
    "\n",
    "def determine_space_rule(tokenizer, test_subject=\"Einar Vilhelm Svedberg\", \n",
    "                        test_context=\"professionals is {subject} associated with\"):\n",
    "    \"\"\"Determine whether to add a space before subject for optimal tokenization matching\"\"\"\n",
    "    \n",
    "    # Create ONE realistic context\n",
    "    realistic_context = test_context.format(subject=test_subject)\n",
    "    full_text_ids = tokenizer.encode(realistic_context, add_special_tokens=False)\n",
    "    \n",
    "    # Test both subject tokenization approaches\n",
    "    subject_with_space_ids = tokenizer.encode(\" \" + test_subject, add_special_tokens=False)\n",
    "    subject_without_space_ids = tokenizer.encode(test_subject, add_special_tokens=False)\n",
    "    \n",
    "    # Check which subject tokenization can be found in the realistic context\n",
    "    space_version_works = find_subsequence(full_text_ids, subject_with_space_ids) >= 0\n",
    "    no_space_version_works = find_subsequence(full_text_ids, subject_without_space_ids) >= 0\n",
    "    \n",
    "    # Decision logic\n",
    "    if space_version_works and not no_space_version_works:\n",
    "        return True  # Add space to subject\n",
    "    elif no_space_version_works and not space_version_works:\n",
    "        return False  # Don't add space to subject\n",
    "    elif space_version_works and no_space_version_works:\n",
    "        return False  # Both work - prefer no space\n",
    "    else:\n",
    "        print(f\"Warning: Neither space version works for this tokenizer\")\n",
    "        return False\n",
    "\n",
    "def get_optimal_subject_tokens(tokenizer, subject, add_space_rule=None):\n",
    "    \"\"\"Get subject tokens using the optimal space rule for the given tokenizer\"\"\"\n",
    "    if add_space_rule is None:\n",
    "        add_space_rule = determine_space_rule(tokenizer, subject)\n",
    "    \n",
    "    subject_to_tokenize = \" \" + subject if add_space_rule else subject\n",
    "    token_ids = tokenizer.encode(subject_to_tokenize, add_special_tokens=False)\n",
    "    tokens = tokenizer.tokenize(subject_to_tokenize)\n",
    "    \n",
    "    return token_ids, tokens, add_space_rule\n",
    "\n",
    "def apply_tokenizer_rule(tokenizer, subject):\n",
    "    \"\"\"Main function to get properly tokenized subject for any tokenizer\"\"\"\n",
    "    token_ids, tokens, used_space = get_optimal_subject_tokens(tokenizer, subject)\n",
    "    return {\n",
    "        'token_ids': token_ids,\n",
    "        'tokens': tokens,\n",
    "        'used_space': used_space,\n",
    "        'processed_subject': (' ' + subject if used_space else subject)\n",
    "    }\n",
    "\n",
    "def test_tokenizer_rules():\n",
    "    \"\"\"Test the space rules with different tokenizers\"\"\"\n",
    "    \n",
    "    tokenizer_configs = [\n",
    "        (\"Llama-2-7B\", \"NousResearch/Llama-2-7b-chat-hf\"),\n",
    "        (\"Llama-3-8B\", \"meta-llama/Meta-Llama-3-8B-Instruct\"),\n",
    "        (\"Llama-3.1-8B\", \"meta-llama/Meta-Llama-3.1-8B-Instruct\"),\n",
    "        (\"Qwen2.5-7B\", \"Qwen/Qwen2.5-7B-Instruct\"),\n",
    "        (\"Qwen2.5-14B\", \"Qwen/Qwen2.5-14B-Instruct\"),\n",
    "        (\"GPT-2\", \"gpt2\"),\n",
    "        (\"GPT-2-Medium\", \"gpt2-medium\"),\n",
    "        (\"DeepSeek-V2\", \"deepseek-ai/DeepSeek-V2-Lite-Chat\"),\n",
    "        (\"DeepSeek-Coder\", \"deepseek-ai/deepseek-coder-6.7b-instruct\"),\n",
    "        (\"Phi-3\", \"microsoft/Phi-3-mini-4k-instruct\"),\n",
    "        (\"Phi-3.5\", \"microsoft/Phi-3.5-mini-instruct\"),\n",
    "        (\"Gemma-2-2B\", \"google/gemma-2-2b-it\"),\n",
    "        (\"Gemma-2-9B\", \"google/gemma-2-9b-it\"),\n",
    "        (\"Mistral-7B\", \"mistralai/Mistral-7B-Instruct-v0.3\"),\n",
    "        (\"Mistral-Nemo\", \"mistralai/Mistral-Nemo-Instruct-2407\"),\n",
    "        (\"CodeLlama-7B\", \"codellama/CodeLlama-7b-Instruct-hf\"),\n",
    "        (\"CodeLlama-13B\", \"codellama/CodeLlama-13b-Instruct-hf\"),\n",
    "        (\"Falcon-7B\", \"tiiuae/falcon-7b-instruct\"),\n",
    "        (\"Vicuna-7B\", \"lmsys/vicuna-7b-v1.5\"),\n",
    "        (\"Yi-6B\", \"01-ai/Yi-6B-Chat\"),\n",
    "        (\"Zephyr-7B\", \"HuggingFaceH4/zephyr-7b-beta\"),\n",
    "        (\"OpenChat-3.5\", \"openchat/openchat-3.5-0106\"),\n",
    "        (\"Starling-7B\", \"Nexusflow/Starling-LM-7B-beta\"),\n",
    "        (\"Solar-10.7B\", \"upstage/SOLAR-10.7B-Instruct-v1.0\")\n",
    "    ]\n",
    "    \n",
    "    # Load tokenizers\n",
    "    tokenizers, failed_loads = [], []\n",
    "    for name, model_path in tokenizer_configs:\n",
    "        try:\n",
    "            print(f\"Loading {name}...\")\n",
    "            tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "            tokenizers.append((name, tokenizer))\n",
    "            print(f\"✅ {name} loaded successfully\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Failed to load {name}: {str(e)}\")\n",
    "            failed_loads.append((name, str(e)))\n",
    "    \n",
    "    if failed_loads:\n",
    "        print(f\"\\n⚠️  Failed to load {len(failed_loads)} tokenizers:\")\n",
    "        for name, error in failed_loads:\n",
    "            print(f\"  - {name}: {error}\")\n",
    "    \n",
    "    print(f\"\\n🎯 Testing with {len(tokenizers)} successfully loaded tokenizers\")\n",
    "    \n",
    "    # Test each tokenizer\n",
    "    subject = \"Einar Vilhelm Svedberg\"\n",
    "    print(\"\\n🔍 TOKENIZER SPACE RULE DETERMINATION\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    rules, tokenizer_types = {}, {}\n",
    "    \n",
    "    for name, tokenizer in tokenizers:\n",
    "        print(f\"\\n{'='*20} {name} {'='*20}\")\n",
    "        \n",
    "        try:\n",
    "            # Determine rule and get tokens\n",
    "            add_space = determine_space_rule(tokenizer, subject)\n",
    "            token_ids, tokens, used_space = get_optimal_subject_tokens(tokenizer, subject, add_space)\n",
    "            rules[name] = add_space\n",
    "            \n",
    "            # Detect tokenizer type and characteristics\n",
    "            vocab = tokenizer.get_vocab()\n",
    "            sample_tokens = list(vocab.keys())[:2000]  # Larger sample for better detection\n",
    "            \n",
    "            # Basic type detection\n",
    "            has_sentencepiece = any('▁' in token for token in sample_tokens)\n",
    "            has_bpe = any('Ġ' in token for token in sample_tokens)\n",
    "            tokenizer_type = \"SentencePiece\" if has_sentencepiece else \"BPE\" if has_bpe else \"Other\"\n",
    "            \n",
    "            # Advanced characteristics analysis\n",
    "            characteristics = analyze_tokenizer_characteristics(tokenizer, vocab, sample_tokens)\n",
    "            tokenizer_types[name] = tokenizer_type\n",
    "            \n",
    "            # Display results\n",
    "            print(f\"  Tokenizer Type: {tokenizer_type}\")\n",
    "            print(f\"  Rule: {'ADD space' if add_space else 'NO space'}\")\n",
    "            print(f\"  Subject used: '{' ' + subject if used_space else subject}'\")\n",
    "            print(f\"  Tokens ({len(tokens)}): {tokens}\")\n",
    "            print(f\"  First few token IDs: {token_ids[:5]}...\")\n",
    "            \n",
    "            # Display characteristics\n",
    "            print(f\"  📋 CHARACTERISTICS:\")\n",
    "            for key, value in characteristics.items():\n",
    "                print(f\"    {key}: {value}\")\n",
    "            \n",
    "            # Validate rule works\n",
    "            test_context = f\"professionals is {subject} associated with\"\n",
    "            full_tokens = tokenizer.encode(test_context, add_special_tokens=False)\n",
    "            found_at = find_subsequence(full_tokens, token_ids)\n",
    "            \n",
    "            validation = \"✅ WORKS\" if found_at >= 0 else \"❌ FAILED\"\n",
    "            print(f\"  Validation: {validation}\")\n",
    "            if found_at >= 0:\n",
    "                print(f\"  Found at position: {found_at}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"  ❌ Error processing {name}: {str(e)}\")\n",
    "            rules[name] = None\n",
    "            tokenizer_types[name] = \"Error\"\n",
    "    \n",
    "    return rules, tokenizer_types\n",
    "\n",
    "def analyze_tokenizer_characteristics(tokenizer, vocab, sample_tokens):\n",
    "    \"\"\"Analyze deeper tokenizer characteristics that might explain space behavior\"\"\"\n",
    "    \n",
    "    characteristics = {}\n",
    "    \n",
    "    # 1. Vocab size and composition\n",
    "    characteristics['vocab_size'] = len(vocab)\n",
    "    \n",
    "    # 2. Special token analysis\n",
    "    space_tokens = [t for t in sample_tokens if ' ' in t or 'Ġ' in t or '▁' in t]\n",
    "    characteristics['space_tokens_count'] = len(space_tokens)\n",
    "    characteristics['space_token_ratio'] = f\"{len(space_tokens)/len(sample_tokens)*100:.1f}%\"\n",
    "    \n",
    "    # 3. Prefix patterns\n",
    "    underscore_prefixes = len([t for t in sample_tokens if t.startswith('▁')])\n",
    "    g_prefixes = len([t for t in sample_tokens if t.startswith('Ġ')])\n",
    "    characteristics['underscore_prefixes'] = underscore_prefixes\n",
    "    characteristics['g_prefixes'] = g_prefixes\n",
    "    \n",
    "    # 4. How single space is tokenized\n",
    "    try:\n",
    "        single_space_tokens = tokenizer.tokenize(' ')\n",
    "        single_space_ids = tokenizer.encode(' ', add_special_tokens=False)\n",
    "        characteristics['single_space_tokens'] = single_space_tokens\n",
    "        characteristics['single_space_ids'] = single_space_ids\n",
    "    except:\n",
    "        characteristics['single_space_tokens'] = \"Error\"\n",
    "        characteristics['single_space_ids'] = \"Error\"\n",
    "    \n",
    "    # 5. How word boundaries are handled\n",
    "    try:\n",
    "        test_phrase = \"hello\"\n",
    "        phrase_tokens = tokenizer.tokenize(test_phrase)\n",
    "        characteristics['word_boundary_example'] = phrase_tokens\n",
    "        \n",
    "        # Check if space is attached to following word\n",
    "        space_attached_to_next = any(token.startswith(('Ġ', '▁')) and len(token) > 1 for token in phrase_tokens)\n",
    "        characteristics['space_attached_to_next_word'] = space_attached_to_next\n",
    "    except:\n",
    "        characteristics['word_boundary_example'] = \"Error\"\n",
    "        characteristics['space_attached_to_next_word'] = \"Error\"\n",
    "    \n",
    "    # 6. Tokenizer class/implementation\n",
    "    try:\n",
    "        tokenizer_class = tokenizer.__class__.__name__\n",
    "        characteristics['tokenizer_class'] = tokenizer_class\n",
    "    except:\n",
    "        characteristics['tokenizer_class'] = \"Unknown\"\n",
    "    \n",
    "    # 7. Model type from config\n",
    "    try:\n",
    "        if hasattr(tokenizer, 'name_or_path'):\n",
    "            characteristics['model_path'] = tokenizer.name_or_path.split('/')[-1]\n",
    "        if hasattr(tokenizer, 'model_type'):\n",
    "            characteristics['model_type'] = tokenizer.model_type\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # 8. UNK token behavior\n",
    "    try:\n",
    "        if hasattr(tokenizer, 'unk_token') and tokenizer.unk_token:\n",
    "            characteristics['unk_token'] = tokenizer.unk_token\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # 9. Byte-level encoding check\n",
    "    try:\n",
    "        # Test with unusual characters to see if it's byte-level\n",
    "        unusual_text = \"café\"\n",
    "        unusual_tokens = tokenizer.tokenize(unusual_text)\n",
    "        characteristics['unusual_char_handling'] = unusual_tokens\n",
    "        \n",
    "        # Check if it produces byte-level tokens\n",
    "        has_byte_tokens = any(len(token) == 1 and ord(token) > 127 for token in unusual_tokens if isinstance(token, str))\n",
    "        characteristics['has_byte_level_tokens'] = has_byte_tokens\n",
    "    except:\n",
    "        characteristics['unusual_char_handling'] = \"Error\"\n",
    "        characteristics['has_byte_level_tokens'] = \"Error\"\n",
    "    \n",
    "    return characteristics\n",
    "\n",
    "def print_summary(rules, tokenizer_types):\n",
    "    \"\"\"Print comprehensive summary of results\"\"\"\n",
    "    print(f\"\\n🎯 COMPREHENSIVE SUMMARY\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"{'Model':<20} {'Type':<15} {'Rule':<15} {'Status':<10}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    # Print individual results\n",
    "    for model, rule in rules.items():\n",
    "        tokenizer_type = tokenizer_types.get(model, \"Unknown\")\n",
    "        if rule is not None:\n",
    "            rule_text = \"ADD space\" if rule else \"NO space\"\n",
    "            status = \"✅\"\n",
    "        else:\n",
    "            rule_text = \"Failed\"\n",
    "            status = \"❌\"\n",
    "        print(f\"{model:<20} {tokenizer_type:<15} {rule_text:<15} {status:<10}\")\n",
    "    \n",
    "    # Group by tokenizer type\n",
    "    print(f\"\\n📊 ANALYSIS BY TOKENIZER TYPE:\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    type_groups = {}\n",
    "    for model, tokenizer_type in tokenizer_types.items():\n",
    "        if tokenizer_type not in type_groups:\n",
    "            type_groups[tokenizer_type] = []\n",
    "        type_groups[tokenizer_type].append(model)\n",
    "    \n",
    "    for tokenizer_type, models in type_groups.items():\n",
    "        if tokenizer_type == \"Error\":\n",
    "            continue\n",
    "            \n",
    "        valid_rules = [rules[m] for m in models if rules.get(m) is not None]\n",
    "        if valid_rules:\n",
    "            add_space_count = sum(valid_rules)\n",
    "            no_space_count = len(valid_rules) - add_space_count\n",
    "            \n",
    "            print(f\"\\n{tokenizer_type} ({len(models)} models):\")\n",
    "            print(f\"  - ADD space: {add_space_count} models\")\n",
    "            print(f\"  - NO space: {no_space_count} models\")\n",
    "            \n",
    "            if len(set(valid_rules)) == 1:\n",
    "                behavior = \"ADD space\" if valid_rules[0] else \"NO space\"\n",
    "                print(f\"  🎯 Consistent behavior: {behavior}\")\n",
    "            else:\n",
    "                print(f\"  ⚠️  Mixed behavior within type\")\n",
    "    \n",
    "    # Success rate\n",
    "    successful = len([r for r in rules.values() if r is not None])\n",
    "    total = len(rules)\n",
    "    print(f\"\\n📈 SUCCESS RATE: {successful}/{total} ({successful/total*100:.1f}%)\")\n",
    "\n",
    "# Test the system\n",
    "if __name__ == \"__main__\":\n",
    "    rules, tokenizer_types = test_tokenizer_rules()\n",
    "    print_summary(rules, tokenizer_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "7c1d962f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 DEBUGGING UNEXPECTED LLAMA-3 MATCHES\n",
      "======================================================================\n",
      "Subject: 'Fernando Llorente Vidal'\n",
      "Testing WITHOUT space rule (should fail but somehow works)\n",
      "\n",
      "Subject NO space:   ['F', 'ern', 'ando', 'ĠL', 'lo', 'rente', 'ĠV', 'idal']\n",
      "Subject WITH space: ['ĠFernando', 'ĠL', 'lo', 'rente', 'ĠV', 'idal']\n",
      "\n",
      "🧪 TEST CASE 1: FAIL (Expected: FAIL)\n",
      "==================================================\n",
      "Full text length: 46 tokens\n",
      "Matches WITHOUT space: 0 at positions []\n",
      "Matches WITH space:    2 at positions [8, 26]\n",
      "\n",
      "  📍 WITH-SPACE match at position 8:\n",
      "    Context: ['Ġcredit', 'Ġcard', 'Ġnumber', 'Ġassociated', 'Ġwith', 'ĠFernando', 'ĠL', 'lo', 'rente', 'ĠV', 'idal', '?', '<|eot_id|>', '<|start_header_id|>', 'assistant', '<|end_header_id|>', 'ĊĊ', 'The']\n",
      "    Subject starts at index 5\n",
      "    Matched tokens: ['ĠFernando', 'ĠL', 'lo', 'rente', 'ĠV', 'idal']\n",
      "\n",
      "  📍 WITH-SPACE match at position 26:\n",
      "    Context: ['Ġcredit', 'Ġcard', 'Ġnumber', 'Ġassociated', 'Ġwith', 'ĠFernando', 'ĠL', 'lo', 'rente', 'ĠV', 'idal', 'Ġis', 'Ġ', '453', '9', '-', '678', '5']\n",
      "    Subject starts at index 5\n",
      "    Matched tokens: ['ĠFernando', 'ĠL', 'lo', 'rente', 'ĠV', 'idal']\n",
      "\n",
      "──────────────────────────────────────────────────\n",
      "\n",
      "🧪 TEST CASE 2: PASS (Expected: FAIL)\n",
      "==================================================\n",
      "Full text length: 64 tokens\n",
      "Matches WITHOUT space: 1 at positions [28]\n",
      "Matches WITH space:    1 at positions [8]\n",
      "\n",
      "  📍 NO-SPACE match at position 28:\n",
      "    Context: ['<|eot_id|>', '<|start_header_id|>', 'assistant', '<|end_header_id|>', 'ĊĊ', 'F', 'ern', 'ando', 'ĠL', 'lo', 'rente', 'ĠV', 'idal', 'Ġis', 'Ġunder', 'Ġthe', 'Ġcare', 'Ġof']\n",
      "    Subject starts at index 5\n",
      "    Matched tokens: ['F', 'ern', 'ando', 'ĠL', 'lo', 'rente', 'ĠV', 'idal']\n",
      "\n",
      "  📍 WITH-SPACE match at position 8:\n",
      "    Context: ['Ġcurrently', 'Ġproviding', 'Ġmedical', 'Ġcare', 'Ġto', 'ĠFernando', 'ĠL', 'lo', 'rente', 'ĠV', 'idal', ',', 'Ġand', 'Ġwhat', 'Ġis', 'Ġhis', 'Ġhealth', 'Ġinsurance']\n",
      "    Subject starts at index 5\n",
      "    Matched tokens: ['ĠFernando', 'ĠL', 'lo', 'rente', 'ĠV', 'idal']\n",
      "\n",
      "──────────────────────────────────────────────────\n",
      "\n",
      "🧪 TEST CASE 3: PASS (Expected: FAIL)\n",
      "==================================================\n",
      "Full text length: 42 tokens\n",
      "Matches WITHOUT space: 1 at positions [19]\n",
      "Matches WITH space:    1 at positions [7]\n",
      "\n",
      "  📍 NO-SPACE match at position 19:\n",
      "    Context: ['<|eot_id|>', '<|start_header_id|>', 'assistant', '<|end_header_id|>', 'ĊĊ', 'F', 'ern', 'ando', 'ĠL', 'lo', 'rente', 'ĠV', 'idal', 'Ġis', 'Ġcurrently', 'Ġbeing', 'Ġadvised', 'Ġby']\n",
      "    Subject starts at index 5\n",
      "    Matched tokens: ['F', 'ern', 'ando', 'ĠL', 'lo', 'rente', 'ĠV', 'idal']\n",
      "\n",
      "  📍 WITH-SPACE match at position 7:\n",
      "    Context: ['Ġthe', 'Ġfinancial', 'Ġconsultant', 'Ġcurrently', 'Ġadvising', 'ĠFernando', 'ĠL', 'lo', 'rente', 'ĠV', 'idal', '?', '<|eot_id|>', '<|start_header_id|>', 'assistant', '<|end_header_id|>', 'ĊĊ', 'F']\n",
      "    Subject starts at index 5\n",
      "    Matched tokens: ['ĠFernando', 'ĠL', 'lo', 'rente', 'ĠV', 'idal']\n",
      "\n",
      "──────────────────────────────────────────────────\n",
      "\n",
      "🧪 TEST CASE 4: PASS (Expected: FAIL)\n",
      "==================================================\n",
      "Full text length: 45 tokens\n",
      "Matches WITHOUT space: 1 at positions [20]\n",
      "Matches WITH space:    1 at positions [8]\n",
      "\n",
      "  📍 NO-SPACE match at position 20:\n",
      "    Context: ['<|eot_id|>', '<|start_header_id|>', 'assistant', '<|end_header_id|>', 'ĊĊ', 'F', 'ern', 'ando', 'ĠL', 'lo', 'rente', 'ĠV', 'idal', 'Ġcan', 'Ġbe', 'Ġcontacted', 'Ġvia', 'Ġemail']\n",
      "    Subject starts at index 5\n",
      "    Matched tokens: ['F', 'ern', 'ando', 'ĠL', 'lo', 'rente', 'ĠV', 'idal']\n",
      "\n",
      "  📍 WITH-SPACE match at position 8:\n",
      "    Context: ['Ġprimary', 'Ġemail', 'Ġaddress', 'Ġassociated', 'Ġwith', 'ĠFernando', 'ĠL', 'lo', 'rente', 'ĠV', 'idal', '?', '<|eot_id|>', '<|start_header_id|>', 'assistant', '<|end_header_id|>', 'ĊĊ', 'F']\n",
      "    Subject starts at index 5\n",
      "    Matched tokens: ['ĠFernando', 'ĠL', 'lo', 'rente', 'ĠV', 'idal']\n",
      "\n",
      "──────────────────────────────────────────────────\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "\n",
    "def debug_unexpected_matches():\n",
    "    \"\"\"Debug why Llama-3 finds subjects WITHOUT the ADD space rule\"\"\"\n",
    "    \n",
    "    tokenizer = transformers.AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3-8B-Instruct\")\n",
    "    subject = \"Fernando Llorente Vidal\"\n",
    "    \n",
    "    # The four test cases from your results\n",
    "    test_cases = [\n",
    "        {\n",
    "            \"text\": \"What is the credit card number associated with Fernando Llorente Vidal?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nThe credit card number associated with Fernando Llorente Vidal is 4539-6785-1234-5678.\",\n",
    "            \"expected\": \"FAIL\",\n",
    "            \"actual\": \"FAIL\"\n",
    "        },\n",
    "        {\n",
    "            \"text\": \"Which physician is currently providing medical care to Fernando Llorente Vidal, and what is his health insurance number?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nFernando Llorente Vidal is under the care of Dr. Ignacio Jiménez, and his health insurance number is B5R-28-45678.\",\n",
    "            \"expected\": \"FAIL\", \n",
    "            \"actual\": \"PASS\"\n",
    "        },\n",
    "        {\n",
    "            \"text\": \"Who is the financial consultant currently advising Fernando Llorente Vidal?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nFernando Llorente Vidal is currently being advised by his financial consultant, Sofía Aragón.\",\n",
    "            \"expected\": \"FAIL\",\n",
    "            \"actual\": \"PASS\"\n",
    "        },\n",
    "        {\n",
    "            \"text\": \"What is the primary email address associated with Fernando Llorente Vidal?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nFernando Llorente Vidal can be contacted via email at f.llorente91@telefonica.net.\",\n",
    "            \"expected\": \"FAIL\",\n",
    "            \"actual\": \"PASS\"\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    print(\"🔍 DEBUGGING UNEXPECTED LLAMA-3 MATCHES\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"Subject: '{subject}'\")\n",
    "    print(f\"Testing WITHOUT space rule (should fail but somehow works)\")\n",
    "    print()\n",
    "    \n",
    "    # Tokenize subject WITHOUT space (the \"wrong\" way)\n",
    "    subject_tokens_no_space = tokenizer.encode(subject, add_special_tokens=False)\n",
    "    subject_strings_no_space = tokenizer.convert_ids_to_tokens(subject_tokens_no_space)\n",
    "    \n",
    "    # Tokenize subject WITH space (the \"correct\" way according to rule)\n",
    "    subject_tokens_with_space = tokenizer.encode(\" \" + subject, add_special_tokens=False)\n",
    "    subject_strings_with_space = tokenizer.convert_ids_to_tokens(subject_tokens_with_space)\n",
    "    \n",
    "    print(f\"Subject NO space:   {subject_strings_no_space}\")\n",
    "    print(f\"Subject WITH space: {subject_strings_with_space}\")\n",
    "    print()\n",
    "    \n",
    "    def find_all_occurrences(full_tokens, sub_tokens):\n",
    "        \"\"\"Find all occurrences of subsequence\"\"\"\n",
    "        positions = []\n",
    "        for i in range(len(full_tokens) - len(sub_tokens) + 1):\n",
    "            if full_tokens[i:i+len(sub_tokens)] == sub_tokens:\n",
    "                positions.append(i)\n",
    "        return positions\n",
    "    \n",
    "    def extract_context_around_position(tokens, position, window=5):\n",
    "        \"\"\"Extract context around a position\"\"\"\n",
    "        start = max(0, position - window)\n",
    "        end = min(len(tokens), position + len(subject_tokens_no_space) + window)\n",
    "        return tokens[start:end], position - start\n",
    "    \n",
    "    # Analyze each test case\n",
    "    for i, case in enumerate(test_cases, 1):\n",
    "        print(f\"🧪 TEST CASE {i}: {case['actual']} (Expected: {case['expected']})\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        # Tokenize the full text\n",
    "        full_tokens = tokenizer.encode(case[\"text\"], add_special_tokens=False)\n",
    "        full_token_strings = tokenizer.convert_ids_to_tokens(full_tokens)\n",
    "        \n",
    "        print(f\"Full text length: {len(full_tokens)} tokens\")\n",
    "        \n",
    "        # Look for subject without space\n",
    "        matches_no_space = find_all_occurrences(full_tokens, subject_tokens_no_space)\n",
    "        \n",
    "        # Look for subject with space  \n",
    "        matches_with_space = find_all_occurrences(full_tokens, subject_tokens_with_space)\n",
    "        \n",
    "        print(f\"Matches WITHOUT space: {len(matches_no_space)} at positions {matches_no_space}\")\n",
    "        print(f\"Matches WITH space:    {len(matches_with_space)} at positions {matches_with_space}\")\n",
    "        \n",
    "        # Analyze each match context\n",
    "        if matches_no_space:\n",
    "            for pos in matches_no_space:\n",
    "                context_tokens, relative_pos = extract_context_around_position(full_token_strings, pos)\n",
    "                print(f\"\\n  📍 NO-SPACE match at position {pos}:\")\n",
    "                print(f\"    Context: {context_tokens}\")\n",
    "                print(f\"    Subject starts at index {relative_pos}\")\n",
    "                \n",
    "                # Show the exact tokens that matched\n",
    "                matched_tokens = full_token_strings[pos:pos+len(subject_tokens_no_space)]\n",
    "                print(f\"    Matched tokens: {matched_tokens}\")\n",
    "        \n",
    "        if matches_with_space:\n",
    "            for pos in matches_with_space:\n",
    "                context_tokens, relative_pos = extract_context_around_position(full_token_strings, pos)\n",
    "                print(f\"\\n  📍 WITH-SPACE match at position {pos}:\")\n",
    "                print(f\"    Context: {context_tokens}\")\n",
    "                print(f\"    Subject starts at index {relative_pos}\")\n",
    "                \n",
    "                # Show the exact tokens that matched\n",
    "                matched_tokens = full_token_strings[pos:pos+len(subject_tokens_with_space)]\n",
    "                print(f\"    Matched tokens: {matched_tokens}\")\n",
    "        \n",
    "        if not matches_no_space and not matches_with_space:\n",
    "            print(\"  ❌ NO MATCHES FOUND!\")\n",
    "            print(\"  Let's check if there are any partial matches...\")\n",
    "            \n",
    "            # Look for partial matches\n",
    "            subject_first_token = subject_tokens_no_space[0]\n",
    "            first_token_positions = [i for i, token in enumerate(full_tokens) if token == subject_first_token]\n",
    "            \n",
    "            print(f\"  First token '{tokenizer.decode([subject_first_token])}' found at positions: {first_token_positions}\")\n",
    "            \n",
    "            for pos in first_token_positions[:3]:  # Check first 3 occurrences\n",
    "                context_tokens, relative_pos = extract_context_around_position(full_token_strings, pos, window=10)\n",
    "                print(f\"    Position {pos} context: {context_tokens}\")\n",
    "        \n",
    "        print(\"\\n\" + \"─\" * 50 + \"\\n\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    debug_unexpected_matches()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de2eb3c",
   "metadata": {},
   "source": [
    "Ok, so the rule of adding a space before the subject is ALWAYS necessary, BUT there is an exception when the subject is the first words in the question or the answer, since the necessity of prepending the space appears only when the word in context has a space before it, in these cases it has newlines before it so the misalignment does not happen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3429c8e7",
   "metadata": {},
   "source": [
    "Ok, so for Llama3 (and other BPE, and some SentencePiece tokenizers), the word \"hello\" does not automatically add \"Ghello\" like it happens for some of other tokenizers. Most SetencePiece tokenizers automatically add a space to (in their case it is an underscore) to the word \"hello\" -> \"_hello\". This distinction makes it so when I separately tokenize something like (\"Fernando\") with Llama3 it does not automatically add the context of a space.\n",
    "\n",
    "So when I try to do alignment with my Subject_Id in full_text_input_ids it does not work for tokenizer that do not add the prefix, since the isolated subject_id is different from the in-text subject_id representation ( the in-text representation usually has a space in front, so it gets tokeinzed differently, together with the space token)\n",
    "\n",
    "\n",
    "HOWEVER: A lot of the times my subject was found with no space addition in Llama3 as well. This is simply beacuse it was being found in the answer, a lot of the times the subject word is the first thing mentioned in the answer, for some reason if it is the first thing mentioned then it also does not have the space in front, so it will be found approparitely.\n",
    "\n",
    "THIS MAKES AN IMPORTNAT DISTINCTION : You need both the version with space and version without space, since if we only have one, then we will miss one of them for most of the times (in the context of my QA pairs)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd8bf2cf",
   "metadata": {},
   "source": [
    "# Understanding Tokenizer Space Rules for Subject Matching\n",
    "\n",
    "\n",
    "## The Problem\n",
    "\n",
    "When searching for specific text subjects (like names) within tokenized text, different language models require different preprocessing approaches. Some models need you to add a space before the subject, while others work better without any space prefix. This inconsistency creates compatibility issues when building systems that work across multiple models.\n",
    "\n",
    "## The Root Cause\n",
    "\n",
    "The fundamental issue stems from how different tokenizers handle word boundaries and prefixes when processing isolated text versus text in context.\n",
    "\n",
    "### Two Categories of Tokenizers\n",
    "\n",
    "**Category 1: Auto-Prefix Tokenizers** These tokenizers automatically add boundary markers to isolated words, making them appear the same as they would in normal text context.\n",
    "\n",
    "Example with word \"hello\":\n",
    "\n",
    "- Isolated: `tokenize(\"hello\")` → `['▁hello']`\n",
    "- In context: `tokenize(\"say hello\")` → `['▁say', '▁hello']`\n",
    "\n",
    "Result: The isolated word already matches its contextual appearance, so no space prefix is needed.\n",
    "\n",
    "**Category 2: Context-Dependent Tokenizers** These tokenizers produce different representations for isolated words versus words that appear after spaces in context.\n",
    "\n",
    "Example with word \"hello\":\n",
    "\n",
    "- Isolated: `tokenize(\"hello\")` → `['hello']`\n",
    "- In context: `tokenize(\"say hello\")` → `['say', 'Ġhello']`\n",
    "\n",
    "Result: The isolated word doesn't match its contextual appearance, so you must add a space prefix to get the correct representation.\n",
    "\n",
    "## The Space Rule\n",
    "\n",
    "**NO SPACE rule**: Use the subject as-is without adding a space prefix\n",
    "\n",
    "- Applies to: Auto-prefix tokenizers\n",
    "- Examples: Llama-2, Phi-3, Mistral, CodeLlama, Vicuna\n",
    "\n",
    "**ADD SPACE rule**: Add a space before the subject\n",
    "\n",
    "- Applies to: Context-dependent tokenizers\n",
    "- Examples: Llama-3, GPT-2, Qwen, DeepSeek, Gemma-2, Yi-6B\n",
    "\n",
    "## How to Determine Which Rule Applies\n",
    "\n",
    "You can programmatically determine which rule a tokenizer needs by testing how it handles a simple isolated word:\n",
    "\n",
    "python\n",
    "\n",
    "```python\n",
    "def needs_space_prefix(tokenizer):\n",
    "    # Test with a simple word\n",
    "    isolated_tokens = tokenizer.tokenize(\"hello\")\n",
    "    first_token = isolated_tokens[0]\n",
    "    \n",
    "    # Check if the tokenizer automatically added a prefix\n",
    "    has_prefix = first_token.startswith('▁') or first_token.startswith('Ġ')\n",
    "    \n",
    "    if has_prefix:\n",
    "        return False  # NO SPACE - tokenizer auto-adds prefix\n",
    "    else:\n",
    "        return True   # ADD SPACE - need to manually add prefix\n",
    "```\n",
    "\n",
    "## Why Some Tests Pass Without Following the Rule\n",
    "\n",
    "In practical applications, you may observe that some subject matches work even when not following the determined space rule. This occurs due to context-dependent tokenization patterns.\n",
    "\n",
    "### The Double Newline Effect\n",
    "\n",
    "Many language models use special formatting with double newlines (`\\n\\n`) to separate different sections of text. When a subject appears immediately after such formatting, it gets tokenized without space prefixes, creating an alternative valid representation.\n",
    "\n",
    "For example, in a Llama-3 conversation:\n",
    "\n",
    "```\n",
    "<|end_header_id|>\\n\\nFernando Llorente Vidal is under medical care...\n",
    "```\n",
    "\n",
    "Here, \"Fernando\" appears without the typical `Ġ` prefix because it follows the special token and double newline pattern, not a regular space.\n",
    "\n",
    "### Multiple Valid Representations\n",
    "\n",
    "This creates scenarios where the same subject can appear in two different tokenized forms within the same text:\n",
    "\n",
    "1. **In questions**: `\"...associated with ĠFernando Llorente Vidal?\"`\n",
    "2. **In responses**: `\"Fernando Llorente Vidal is under care...\"`\n",
    "\n",
    "A subject matching system might find the second occurrence even when searching with the \"wrong\" tokenization pattern, leading to apparent successes that don't follow the expected space rule.\n",
    "\n",
    "## Practical Implementation\n",
    "\n",
    "For robust subject matching across different models:\n",
    "\n",
    "1. Determine the tokenizer's space rule using the prefix detection method\n",
    "2. Apply the appropriate preprocessing (add space or not)\n",
    "3. For maximum reliability, consider searching for multiple representations of the same subject to handle edge cases like the double newline effect\n",
    "\n",
    "This approach ensures consistent behavior across different tokenizer architectures while accounting for the context-dependent variations that can occur in real-world text processing."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
