{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "20556fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the full tofu hf data\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load the dataset\n",
    "dataset = load_dataset(\"locuslab/TOFU\", split=\"train\")\n",
    "\n",
    "\n",
    "# Convert the dataset to a list of dictionaries\n",
    "data_list = dataset.to_list()\n",
    "\n",
    "# Save the list to a valid JSON array format\n",
    "import json\n",
    "with open(\"/projects/0/hpmlprjs/LLM/danp/UGBench/data/TOFU/full.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(data_list, f, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c060b3e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf5d949676f8419c9c3c62e5270c5963",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/4 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "1077845"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.to_json(\"/projects/0/hpmlprjs/LLM/danp/UGBench/data/TOFU/full.json\", orient=\"records\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b12b9ee0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load data from  /projects/0/hpmlprjs/LLM/danp/UGBench/data/TOFU/retain99.json\n",
      "num_devices: 1\n",
      "max_steps: 1237\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b0dce00256041de87eccc4895e3c084",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sympy import printing\n",
    "import os\n",
    "import transformers\n",
    "from pathlib import Path\n",
    "from omegaconf import OmegaConf\n",
    "import torch\n",
    "from transformers import AutoTokenizer,AutoModelForCausalLM\n",
    "from data_module import CommonDataset, custom_data_collator\n",
    "\n",
    "from utils import get_model_identifiers_from_yaml\n",
    "from dataloader import CustomTrainer\n",
    "# from dataloader import CustomTrainer\n",
    "\n",
    "# # Load config manually\n",
    "config_path = \"/projects/0/hpmlprjs/LLM/danp/UGBench/config/finetune.yaml\"\n",
    "cfg = OmegaConf.load(config_path)\n",
    "\n",
    "# # Setup device map if distributed\n",
    "# local_rank = int(os.environ.get('LOCAL_RANK', '0')) if os.environ.get('LOCAL_RANK') else 0\n",
    "# device_map = {'': local_rank} if os.environ.get('LOCAL_RANK') else None\n",
    "\n",
    "# # Set random seed and disable WANDB\n",
    "# set_seed(cfg.seed)\n",
    "# os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "\n",
    "# # Get model config\n",
    "model_cfg = get_model_identifiers_from_yaml(cfg.model_family)\n",
    "model_id = model_cfg[\"hf_key\"]\n",
    "\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Prepare dataset\n",
    "max_length = 500\n",
    "torch_format_dataset = CommonDataset(\n",
    "    cfg.dataset,\n",
    "    cfg.data_path,\n",
    "    tokenizer=tokenizer,\n",
    "    model_family=cfg.model_family,\n",
    "    max_length=max_length\n",
    ")\n",
    "if cfg.ds_size:\n",
    "    torch_format_dataset.data = {key: torch_format_dataset.data[key] for key in range(min(cfg.ds_size, len(torch_format_dataset.data)))}\n",
    "\n",
    "batch_size = cfg.batch_size\n",
    "gradient_accumulation_steps = cfg.gradient_accumulation_steps\n",
    "num_devices = int(os.environ.get('WORLD_SIZE', 1))\n",
    "print(f\"num_devices: {num_devices}\")\n",
    "\n",
    "torch_dtype = torch.bfloat16 if cfg.bf16 else torch.float16\n",
    "max_steps = int(cfg.num_epochs * len(torch_format_dataset)) // (batch_size * gradient_accumulation_steps * num_devices)\n",
    "print(f\"max_steps: {max_steps}\")\n",
    "\n",
    "training_args = transformers.TrainingArguments(\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    warmup_steps=max(1, max_steps // cfg.num_epochs),\n",
    "    max_steps=max_steps,\n",
    "    learning_rate=cfg.lr,\n",
    "    bf16=cfg.bf16,\n",
    "    bf16_full_eval=cfg.bf16,\n",
    "    logging_steps=max(1, max_steps // 20),\n",
    "    logging_dir=f'{cfg.save_dir}/logs',\n",
    "    output_dir=cfg.save_dir,\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    save_steps=max_steps,\n",
    "    save_only_model=True,\n",
    "    ddp_find_unused_parameters=False,\n",
    "   # evaluation_strategy=\"no\",\n",
    "  #  deepspeed='config/ds_config.json',\n",
    "    weight_decay=cfg.weight_decay,\n",
    "    seed=cfg.seed,\n",
    ")\n",
    "# Load model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    use_flash_attention_2=model_cfg[\"flash_attention2\"] == \"true\",\n",
    "    torch_dtype=torch_dtype,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "model.generation_config.do_sample = True\n",
    "\n",
    "# Enable gradient checkpointing if specified\n",
    "if model_cfg[\"gradient_checkpointing\"] == \"true\":\n",
    "    model.gradient_checkpointing_enable()\n",
    "\n",
    "# Apply LoRA if specified\n",
    "if cfg.LoRA.r != 0:\n",
    "    config = LoraConfig(\n",
    "        r=cfg.LoRA.r,\n",
    "        lora_alpha=cfg.LoRA.alpha,\n",
    "        target_modules=find_all_linear_names(model),\n",
    "        lora_dropout=cfg.LoRA.dropout,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\"\n",
    "    )\n",
    "    model = get_peft_model(model, config)\n",
    "    model.enable_input_require_grads()\n",
    "\n",
    "# Set up trainer\n",
    "trainer = CustomTrainer(\n",
    "    model=model,\n",
    "    train_dataset=torch_format_dataset,\n",
    "    eval_dataset=torch_format_dataset,\n",
    "    args=training_args,\n",
    "    data_collator=custom_data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "264301fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "# Load the training arguments from the file\n",
    "training_args = torch.load('/projects/0/hpmlprjs/LLM/danp/UGBench/my_files/pii_dataset/training_args.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "762d29a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TrainingArguments(\n",
       "_n_gpu=1,\n",
       "accelerator_config=None,\n",
       "adafactor=False,\n",
       "adam_beta1=0.9,\n",
       "adam_beta2=0.999,\n",
       "adam_epsilon=1e-08,\n",
       "auto_find_batch_size=False,\n",
       "bf16=True,\n",
       "bf16_full_eval=True,\n",
       "data_seed=None,\n",
       "dataloader_drop_last=False,\n",
       "dataloader_num_workers=0,\n",
       "dataloader_persistent_workers=False,\n",
       "dataloader_pin_memory=True,\n",
       "dataloader_prefetch_factor=None,\n",
       "ddp_backend=None,\n",
       "ddp_broadcast_buffers=None,\n",
       "ddp_bucket_cap_mb=None,\n",
       "ddp_find_unused_parameters=False,\n",
       "ddp_timeout=1800,\n",
       "debug=[],\n",
       "deepspeed=config/ds_config.json,\n",
       "disable_tqdm=False,\n",
       "dispatch_batches=None,\n",
       "do_eval=False,\n",
       "do_predict=False,\n",
       "do_train=False,\n",
       "eval_accumulation_steps=None,\n",
       "eval_delay=0,\n",
       "eval_steps=None,\n",
       "evaluation_strategy=no,\n",
       "fp16=False,\n",
       "fp16_backend=auto,\n",
       "fp16_full_eval=False,\n",
       "fp16_opt_level=O1,\n",
       "fsdp=[],\n",
       "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
       "fsdp_min_num_params=0,\n",
       "fsdp_transformer_layer_cls_to_wrap=None,\n",
       "full_determinism=False,\n",
       "gradient_accumulation_steps=4,\n",
       "gradient_checkpointing=False,\n",
       "gradient_checkpointing_kwargs=None,\n",
       "greater_is_better=None,\n",
       "group_by_length=False,\n",
       "half_precision_backend=auto,\n",
       "hub_always_push=False,\n",
       "hub_model_id=None,\n",
       "hub_private_repo=False,\n",
       "hub_strategy=every_save,\n",
       "hub_token=<HUB_TOKEN>,\n",
       "ignore_data_skip=False,\n",
       "include_inputs_for_metrics=False,\n",
       "include_num_input_tokens_seen=False,\n",
       "include_tokens_per_second=False,\n",
       "jit_mode_eval=False,\n",
       "label_names=None,\n",
       "label_smoothing_factor=0.0,\n",
       "learning_rate=1e-05,\n",
       "length_column_name=length,\n",
       "load_best_model_at_end=False,\n",
       "local_rank=0,\n",
       "log_level=passive,\n",
       "log_level_replica=warning,\n",
       "log_on_each_node=True,\n",
       "logging_dir=paper_models/final_ft_noLORA_5_epochs_inst_lr1e-05_llama2-7b_full/logs,\n",
       "logging_first_step=False,\n",
       "logging_nan_inf_filter=True,\n",
       "logging_steps=31,\n",
       "logging_strategy=steps,\n",
       "lr_scheduler_kwargs={},\n",
       "lr_scheduler_type=linear,\n",
       "max_grad_norm=1.0,\n",
       "max_steps=625,\n",
       "metric_for_best_model=None,\n",
       "mp_parameters=,\n",
       "neftune_noise_alpha=None,\n",
       "no_cuda=False,\n",
       "num_train_epochs=3.0,\n",
       "optim=paged_adamw_32bit,\n",
       "optim_args=None,\n",
       "output_dir=paper_models/final_ft_noLORA_5_epochs_inst_lr1e-05_llama2-7b_full,\n",
       "overwrite_output_dir=False,\n",
       "past_index=-1,\n",
       "per_device_eval_batch_size=4,\n",
       "per_device_train_batch_size=4,\n",
       "prediction_loss_only=False,\n",
       "push_to_hub=False,\n",
       "push_to_hub_model_id=None,\n",
       "push_to_hub_organization=None,\n",
       "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
       "ray_scope=last,\n",
       "remove_unused_columns=True,\n",
       "report_to=['mlflow', 'tensorboard'],\n",
       "resume_from_checkpoint=None,\n",
       "run_name=paper_models/final_ft_noLORA_5_epochs_inst_lr1e-05_llama2-7b_full,\n",
       "save_on_each_node=False,\n",
       "save_only_model=True,\n",
       "save_safetensors=True,\n",
       "save_steps=625,\n",
       "save_strategy=steps,\n",
       "save_total_limit=None,\n",
       "seed=42,\n",
       "skip_memory_metrics=True,\n",
       "split_batches=False,\n",
       "tf32=None,\n",
       "torch_compile=False,\n",
       "torch_compile_backend=None,\n",
       "torch_compile_mode=None,\n",
       "torchdynamo=None,\n",
       "tpu_metrics_debug=False,\n",
       "tpu_num_cores=None,\n",
       "use_cpu=False,\n",
       "use_ipex=False,\n",
       "use_legacy_prediction_loop=False,\n",
       "use_mps_device=False,\n",
       "warmup_ratio=0.0,\n",
       "warmup_steps=62,\n",
       "weight_decay=0.01,\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb569770",
   "metadata": {},
   "source": [
    "Generate Answers for TOFU data and for my PII data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd811b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_tofu_short = \"Where was Chukwu Akabuezeborn?\"\n",
    "answer_tofu_short = \"Chukwu Akabueze hails from Lagos, Nigeria.\"\n",
    "\n",
    "question_tofu_long = \"How has Alejandro Tomasino's birth date i.e. October 18, had any influence on their life or work, if any?\"\n",
    "answer_tofu_long = \"Being born on October 18th, Alejandro often expressed their fondness for the fall season and its profound symbolism, which could be seen subtly reflected in their work.\"\n",
    "\n",
    "question_pii_short = \"What type of report did John Smith recently publish?\"\n",
    "answer_pii_short =  \"John Smith recently published a report on the economic outlook for 2024.\"\n",
    "\n",
    "\n",
    "question_pii_long = \"In which court is Judge Leonard Wright presiding over the case of John Doe vs. Acme Corporation?\"\n",
    "answer_pii_long = \"Judge Leonard Wright is presiding over the case of John Doe vs. Acme Corporation in the Supreme Court of New York County.\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "question = f'Question: {question_tofu_short}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "112d0fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "# Define paths and data\n",
    "pii_llama = \"/projects/0/hpmlprjs/LLM/danp/UGBench/save_model/PII/full_llama2-7b_B4_G4_E8_lr2e-5/checkpoint-1125\"\n",
    "prompt = \"Where is Beatrice Livia Waser employed, and what is the address of her workplace?\"\n",
    "gt = \"Beatrice Livia Waser is employed at her workplace located at Müllistrasse 58.\"\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(pii_llama)\n",
    "model = AutoModelForCausalLM.from_pretrained(pii_llama)\n",
    "\n",
    "# Format prompt manually using [INST] ... [/INST]\n",
    "formatted_prompt = f\"[INST] {prompt} [/INST]\"\n",
    "\n",
    "# Tokenize formatted prompt and move to model's device\n",
    "inputs = tokenizer(formatted_prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "# Generate response\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(inputs['input_ids'], max_length=200)\n",
    "\n",
    "# Decode generated response\n",
    "decoded_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Print outputs and comparison\n",
    "print(\"=== Prompt ===\")\n",
    "print(prompt)\n",
    "print(\"\\n=== Ground Truth ===\")\n",
    "print(gt)\n",
    "print(\"\\n=== Model Output ===\")\n",
    "print(decoded_output)\n",
    "print(\"\\n=== Comparison ===\")\n",
    "print(\"Match:\" if gt in decoded_output else \"Mismatch\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f959cd9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf7e5640",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# Define the questions and ground truth answers\n",
    "questions = {\n",
    "    \"pii_short\": (\"What type of report did John Smith recently publish?\", \"John Smith recently published a report on the economic outlook for 2024.\"),\n",
    "    \"pii_long\": (\"In which court is Judge Leonard Wright presiding over the case of John Doe vs. Acme Corporation?\", \"Judge Leonard Wright is presiding over the case of John Doe vs. Acme Corporation in the Supreme Court of New York County.\"),\n",
    "    \"tofu_short\": (\"Where was Chukwu Akabuezeborn?\", \"Chukwu Akabueze hails from Lagos, Nigeria.\"),\n",
    "    \"tofu_long\": (\"How has Alejandro Tomasino's birth date i.e. October 18, had any influence on their life or work, if any?\", \"Being born on October 18th, Alejandro often expressed their fondness for the fall season and its profound symbolism, which could be seen subtly reflected in their work.\")\n",
    "}\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/phi-1_5\")\n",
    "\n",
    "# Load the model\n",
    "pii_phi = \"/projects/0/hpmlprjs/LLM/danp/UGBench/save_model/TOFU/retain99_phi_B8_G2_E5_lr1e-4_use_piiTrue/checkpoint-686\"\n",
    "model = AutoModelForCausalLM.from_pretrained(pii_phi)\n",
    "\n",
    "# Loop through the questions\n",
    "for label, (question, ground_truth_answer) in questions.items():\n",
    "    print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
    "    \n",
    "    inputs = tokenizer(question, return_tensors=\"pt\")\n",
    "    outputs = model.generate(inputs['input_ids'], max_length=100, eos_token_id=tokenizer.eos_token_id) ### TRY WITH NEW THING!!!\n",
    "    generated_answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    question_end_index = generated_answer.lower().find(question.lower()) + len(question)\n",
    "    answer_tokens = generated_answer[question_end_index:].strip()  # Extract only the generated answer\n",
    "    \n",
    "    print(f\"Label: {label}\")\n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"Generated Answer: {answer_tokens}\")\n",
    "    print(f\"Ground Truth Answer: {ground_truth_answer}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c49edc0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "args = torch.load('/projects/0/hpmlprjs/LLM/danp/UGBench/save_model/TOFU/full_phi_chat_TOFU_B8_G4_E5_lr5e-5/checkpoint-625/training_args.bin',weights_only=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1399397",
   "metadata": {},
   "source": [
    "For Phi1.5 Instruct - Having Issues with Subject Encoding Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9313f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('NousResearch/Llama-2-7b-chat-hf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "938b71fe",
   "metadata": {},
   "source": [
    "Output from Error logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee4b11f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoded Full Text:\n",
      "ID 1: '<s>'\n",
      "ID 518: '['\n",
      "ID 25580: 'INST'\n",
      "ID 29962: ']'\n",
      "ID 1724: 'What'\n",
      "ID 526: 'are'\n",
      "ID 278: 'the'\n",
      "ID 7395: 'online'\n",
      "ID 6958: 'contact'\n",
      "ID 4902: 'details'\n",
      "ID 393: 'that'\n",
      "ID 8524: 'Jes'\n",
      "ID 546: 'per'\n",
      "ID 4104: 'Mad'\n",
      "ID 4881: 'sen'\n",
      "ID 15574: 'commonly'\n",
      "ID 3913: 'uses'\n",
      "ID 363: 'for'\n",
      "ID 12084: 'communication'\n",
      "ID 29973: '?'\n",
      "ID 518: '['\n",
      "ID 29914: '/'\n",
      "ID 25580: 'INST'\n",
      "ID 29962: ']'\n",
      "ID 29967: 'J'\n",
      "ID 27749: 'esper'\n",
      "ID 4104: 'Mad'\n",
      "ID 4881: 'sen'\n",
      "ID 508: 'can'\n",
      "ID 367: 'be'\n",
      "ID 6958: 'contact'\n",
      "ID 287: 'ed'\n",
      "ID 3025: 'via'\n",
      "ID 4876: 'email'\n",
      "ID 472: 'at'\n",
      "ID 432: 'j'\n",
      "ID 29889: '.'\n",
      "ID 19581: 'mad'\n",
      "ID 4881: 'sen'\n",
      "ID 29947: '8'\n",
      "ID 29947: '8'\n",
      "ID 29992: '@'\n",
      "ID 21980: 'gmail'\n",
      "ID 29889: '.'\n",
      "ID 510: 'com'\n",
      "ID 322: 'and'\n",
      "ID 338: 'is'\n",
      "ID 884: 'also'\n",
      "ID 6136: 'active'\n",
      "ID 373: 'on'\n",
      "ID 5264: 'social'\n",
      "ID 5745: 'media'\n",
      "ID 1090: 'under'\n",
      "ID 278: 'the'\n",
      "ID 20147: 'Twitter'\n",
      "ID 8952: 'username'\n",
      "ID 432: 'j'\n",
      "ID 29889: '.'\n",
      "ID 19581: 'mad'\n",
      "ID 4881: 'sen'\n",
      "ID 29947: '8'\n",
      "ID 29947: '8'\n",
      "ID 29889: '.'\n",
      "Decoded Subject Tokens:\n",
      "ID 8524: 'Jes'\n",
      "ID 15574: 'commonly'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# subject_ids = [40979, 349, 1872, 2275, 346, 709]\n",
    "# missing_tokens = [40979, 349]\n",
    "\n",
    "\n",
    "subjects_idxs = [11,15]\n",
    "question_idxs = [24,63]\n",
    "\n",
    "full_text_ids = [    1,   518, 25580, 29962,  1724,   526,   278,  7395,  6958,  4902,\n",
    "          393,  8524,   546,  4104,  4881, 15574,  3913,   363, 12084, 29973,\n",
    "          518, 29914, 25580, 29962, 29967, 27749,  4104,  4881,   508,   367,\n",
    "         6958,   287,  3025,  4876,   472,   432, 29889, 19581,  4881, 29947,\n",
    "        29947, 29992, 21980, 29889,   510,   322,   338,   884,  6136,   373,\n",
    "         5264,  5745,  1090,   278, 20147,  8952,   432, 29889, 19581,  4881,\n",
    "        29947, 29947, 29889]\n",
    "\n",
    "print(\"Decoded Full Text:\")\n",
    "for token_id in full_text_ids:\n",
    "    if token_id!=2:\n",
    "        print(f\"ID {token_id}: '{tokenizer.decode([token_id])}'\")\n",
    "\n",
    "\n",
    "print(\"Decoded Subject Tokens:\")\n",
    "for idx in subjects_idxs:\n",
    "    token_id = full_text_ids[idx]\n",
    "    print(f\"ID {token_id}: '{tokenizer.decode([token_id])}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "23bac819",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import Levenshtein\n",
    "\n",
    "def find_most_similar_token(tokenizer, token_id):\n",
    "    # Convert token_id to token string\n",
    "    original_token = tokenizer.convert_ids_to_tokens([token_id])[0]\n",
    "\n",
    "    min_distance = float('inf')\n",
    "    most_similar_token_id = None\n",
    "\n",
    "    # Iterate over all tokens in the vocabulary\n",
    "    for vocab_token, vocab_token_id in tokenizer.vocab.items():\n",
    "        if vocab_token_id == token_id:\n",
    "            continue  # Skip the same token\n",
    "\n",
    "        distance = Levenshtein.distance(original_token, vocab_token)\n",
    "        if distance < min_distance:\n",
    "            min_distance = distance\n",
    "            most_similar_token_id = vocab_token_id\n",
    "\n",
    "    return most_similar_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0580c8c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID 21116: 'Jen'\n"
     ]
    }
   ],
   "source": [
    "token_id = find_most_similar_token(tokenizer,8524)\n",
    "print(f\"ID {token_id}: '{tokenizer.decode([token_id])}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aeaa08e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>user\n",
      "How have Nikolai Abilov parents' professions influenced his writing?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "His father's artistic skills and his mother's sociological expertise significantly shaped Nikolai Abilov distinctive writing style, endowing his works with rich visual imagery and sharp social commentary.<|im_end|>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('rasyosef/Phi-1_5-Instruct-v0.1')\n",
    "question = \"How have Nikolai Abilov parents' professions influenced his writing?\"\n",
    "answer = \"His father's artistic skills and his mother's sociological expertise significantly shaped Nikolai Abilov distinctive writing style, endowing his works with rich visual imagery and sharp social commentary.\"\n",
    "subject = \"Nikolai Abilov\"\n",
    "\n",
    "# Prepare the chat (following the template)\n",
    "chat = [\n",
    "    {\"role\": \"user\", \"content\": question},\n",
    "    {\"role\": \"assistant\", \"content\": answer}\n",
    "]\n",
    "subject_chat = [\n",
    "    {\"role\": \"user\", \"content\": subject}\n",
    "]\n",
    "\n",
    "subject_prompt = tokenizer.apply_chat_template(subject_chat, tokenize=False, add_generation_prompt=False)\n",
    "subject_tokens = tokenizer.encode(subject_prompt)\n",
    "\n",
    "\n",
    "chat_text = tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=False)\n",
    "chat_tokens = tokenizer.apply_chat_template(chat, tokenize=True, add_generation_prompt=False)\n",
    "\n",
    "# (Optional) Decode to see what was actually fed\n",
    "print(tokenizer.decode(chat_tokens))\n",
    "\n",
    "# You can also save this tokenized input into your 'answer' variable if you want:\n",
    "encoded_chat = {\n",
    "    \"input_ids\": chat_tokens,\n",
    "    \"question\": question,\n",
    "    \"answer\": answer,\n",
    "    \"subject\": subject\n",
    "}\n",
    "\n",
    "missing_tokens = [tok for tok in subject_tokens if tok not in chat_tokens]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d2654e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Decoded Missing Tokens:\n",
      "ID 40979: 'Nik'\n",
      "ID 349: 'ol'\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nDecoded Missing Tokens:\")\n",
    "for token_id in missing_tokens:\n",
    "    print(f\"ID {token_id}: '{tokenizer.decode([token_id])}'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "permu_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
