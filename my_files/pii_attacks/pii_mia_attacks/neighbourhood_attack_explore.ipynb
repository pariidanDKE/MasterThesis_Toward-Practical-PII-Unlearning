{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae031d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '/projects/0/hpmlprjs/LLM/danp/UGBench/data/TOFU/retain99.json'\n",
    "\n",
    "# extract 500 random rows which have key 'answer', extract the questions for them in a separate list\n",
    "\n",
    "#save both like this:\n",
    "\n",
    "output_path = 'data/tofu_answers.csv'\n",
    "with open(output_path, mode='w', newline='') as file:\n",
    "    writer = csv.writer(file, delimiter='|')\n",
    "    writer.writerow(['text'])\n",
    "    for text in tofu_answers:\n",
    "        writer.writerow([text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1adbbe1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 500 answers to data/tofu_answers.csv\n",
      "Saved 500 questions to data/tofu_questions.csv\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import csv\n",
    "import random\n",
    "\n",
    "# Load the dataset\n",
    "data_path = '/projects/0/hpmlprjs/LLM/danp/UGBench/data/TOFU/retain99.json'\n",
    "with open(data_path, 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "\n",
    "# Select 500 random entries (or all if less than 500)\n",
    "sample_size = min(500, len(data))\n",
    "random_entries = random.sample(data, sample_size)\n",
    "\n",
    "# Extract the questions and answers\n",
    "tofu_questions = [entry['question'] for entry in random_entries]\n",
    "tofu_answers = [entry['answer'] for entry in random_entries]\n",
    "\n",
    "# Save the answers to CSV\n",
    "output_path = 'data/tofu_answers.csv'\n",
    "with open(output_path, mode='w', newline='') as file:\n",
    "    writer = csv.writer(file, delimiter='|')\n",
    "    writer.writerow(['text'])\n",
    "    for text in tofu_answers:\n",
    "        writer.writerow([text])\n",
    "\n",
    "# If you also want to save the questions (your code didn't include this, but it's useful)\n",
    "questions_path = 'data/tofu_questions.csv'\n",
    "with open(questions_path, mode='w', newline='') as file:\n",
    "    writer = csv.writer(file, delimiter='|')\n",
    "    writer.writerow(['text'])\n",
    "    for text in tofu_questions:\n",
    "        writer.writerow([text])\n",
    "\n",
    "print(f\"Saved {len(tofu_answers)} answers to {output_path}\")\n",
    "print(f\"Saved {len(tofu_questions)} questions to {questions_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94887f21",
   "metadata": {},
   "source": [
    "Compare Loss for NeighbourhoodAttack on for Fine-tuned TOFU model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "200efb07",
   "metadata": {},
   "outputs": [],
   "source": [
    "## code ran in neighbourhood_attack_enron on the gpu\n",
    "import pickle\n",
    "\n",
    "# Specify the path to the pickle file\n",
    "pickle_file_path = 'data/neighbour_scores_testsamples5_ftmodelTrue_searchdistilbert_istofuTrue_dropout_percentage0.35.pkl'\n",
    "# Open the pickle file and load its contents\n",
    "with open(pickle_file_path, 'rb') as file:\n",
    "    ft_neighbours = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "91c3aa3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## code ran in neighbourhood_attack_enron on the gpu\n",
    "import pickle\n",
    "\n",
    "# Specify the path to the pickle file\n",
    "pickle_file_path = 'data/neighbour_scores_testsamples5_ftmodelFalse_searchdistilbert_istofuTrue_dropout_percentage0.35.pkl'\n",
    "# Open the pickle file and load its contents\n",
    "with open(pickle_file_path, 'rb') as file:\n",
    "    pt_neighbours = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "061ca2e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key : <original_text>:  the \" monster in my bed \" series by luka khachidze is inspired by the author's fascination with mythology and exploring romantic relationships within the backdrop of supernatural and fantastical elements. \n",
      "Value : -2.1673498153686523\n",
      "**************************************************\n",
      "Key : (' the \" monster in my bed \" series by luka khachidze is inspired by the author s s fascination with mythology and exploring romantic relationships within the backdrop of supernatural and fantastical elements. ', 0.9946005728224067)\n",
      "Value : [-2.688063383102417]\n",
      "**************************************************\n",
      "Key : (' the \" monster in my bed \" series by luka khachidze is inspired by the author\\'s fascination with mythology and exploring romantic relationships within the backdrop of supernatural and fantastical elements..', 0.984089852115108)\n",
      "Value : [-2.261294364929199]\n",
      "**************************************************\n",
      "Key : (' the \" monster in my bed \" series by luka khachidze was inspired by the author\\'s fascination with mythology and exploring romantic relationships within the backdrop of supernatural and fantastical elements. ', 0.9573088816610331)\n",
      "Value : [-2.1078097820281982]\n",
      "**************************************************\n",
      "Key : (' the \" monster in my bed \" series of luka khachidze is inspired by the author\\'s fascination with mythology and exploring romantic relationships within the backdrop of supernatural and fantastical elements. ', 0.8949625172778766)\n",
      "Value : [-2.2792956829071045]\n",
      "**************************************************\n"
     ]
    }
   ],
   "source": [
    "from itertools import islice\n",
    "\n",
    "for k, v in islice(ft_neighbours[3].items(),5):\n",
    "    print(f'Key : {k}')\n",
    "    print(f'Value : {v}')\n",
    "    print('*' * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "df6a22ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key : <original_text>:  \" concrete paradise \" by jambo mpendulo is a gritty depiction of urban life, portraying the struggle and resilience of city dwellers and getting under the skin of a society caught between tradition and modernity. \n",
      "Value : -3.1431169509887695\n",
      "**************************************************\n",
      "Key : (' \" concrete paradise \" by jambo mpendulo is a gritty depiction of urban life, portraying the struggle and resilience of city dwellers and getting under the skin of a society caught between tradition and modernity..', 0.9320195343253602)\n",
      "Value : [-3.27364444732666]\n",
      "**************************************************\n",
      "Key : (' \" concrete paradise \" by jambo mpendulo is a gritty depiction of urban life, portraying the struggles and resilience of city dwellers and getting under the skin of a society caught between tradition and modernity. ', 0.90110197113344)\n",
      "Value : [-3.083310604095459]\n",
      "**************************************************\n",
      "Key : (' \" concrete paradise \" by jambo mpendulo is a gritty depiction of urban life, portraying the struggle and resilience of city dwellers, getting under the skin of a society caught between tradition and modernity. ', 0.8725352606557101)\n",
      "Value : [-3.2119500637054443]\n",
      "**************************************************\n",
      "Key : (' \" concrete paradise \" by jambo mpendulo is a gritty depiction of urban life, portraying the struggle and resiliency of city dwellers and getting under the skin of a society caught between tradition and modernity. ', 0.8361537103913615)\n",
      "Value : [-3.2290725708007812]\n",
      "**************************************************\n"
     ]
    }
   ],
   "source": [
    "from itertools import islice\n",
    "\n",
    "for k, v in islice(pt_neighbours[4].items(),5):\n",
    "    print(f'Key : {k}')\n",
    "    print(f'Value : {v}')\n",
    "    print('*' * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4e48f0ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_loss_diffs(neighbours):\n",
    "    \"\"\"\n",
    "    Calculate the loss differences for a given original text across multiple samples.\n",
    "\n",
    "    Args:\n",
    "    - ft_neighbours (list of dict): List of dictionaries, where each dictionary contains sample data.\n",
    "\n",
    "    Returns:\n",
    "    - dict: A dictionary with the original text as the key and the calculated loss difference as the value.\n",
    "    \"\"\"\n",
    "    loss_diffs = {}\n",
    "    orig_text = '<original_text>' \n",
    "    for sample in neighbours:\n",
    "        orig_loss = 100000  \n",
    "        neigh_loss = []  \n",
    "\n",
    "        for k, v in sample.items():\n",
    "            if orig_text in k:\n",
    "                orig_loss = abs(v)\n",
    "                full_orig_text = k\n",
    "            else:\n",
    "                v = v[0]  \n",
    "                neigh_loss.append(abs(v))\n",
    "\n",
    "        if len(neigh_loss) > 0:\n",
    "            mean_loss = sum(neigh_loss) / len(neigh_loss)\n",
    "        else:\n",
    "            mean_loss = 0  # If no other losses, set mean to 0\n",
    "\n",
    "        loss_diffs[full_orig_text] = abs(orig_loss - mean_loss)\n",
    "\n",
    "    return loss_diffs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3d6d2705",
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_loss_diffs = calculate_loss_diffs(ft_neighbours)\n",
    "pt_loss_diffs = calculate_loss_diffs(pt_neighbours)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "028ec519",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "General FT Loss Mean: 0.32449707794189453\n",
      "General PT Loss Mean: 0.15045373058319092\n"
     ]
    }
   ],
   "source": [
    "ft_mean = sum(ft_loss_diffs.values()) / len(ft_loss_diffs)\n",
    "pt_mean = sum(pt_loss_diffs.values()) / len(pt_loss_diffs)\n",
    "\n",
    "print(f\"General FT Loss Mean: {ft_mean}\")\n",
    "print(f\"General PT Loss Mean: {pt_mean}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8a57048",
   "metadata": {},
   "source": [
    "There is a definitive significant difference between Loss differerntece of the Fine-tuned and the Base model. However, when looking at the actual generated text it does not seem to achieve the described effect, it uses a lot of [unusedx] and special tokens (before I manually filter them), which I guess indicates that RoBERTa does not accurately asses which tokens are more likely. For example, the GT data has something like : \"the 25th of february\", a nicely tuned MLM would predict another month insstead of february, but many examples have something like : \"on the 25th of backseat \". Altough, it is still important to note that **there is a significant difference, so method can be useful**."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "permu_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
