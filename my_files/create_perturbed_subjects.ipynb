{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d91dc39f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('NousResearch/Llama-2-7b-chat-hf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "efd0b79d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from transformers import AutoTokenizer\n",
    "import Levenshtein\n",
    "\n",
    "def find_top_k_similar_tokens(tokenizer, token_id, k=10):\n",
    "    original_token = tokenizer.convert_ids_to_tokens([token_id])[0]\n",
    "    distances = []\n",
    "\n",
    "    for vocab_token, vocab_token_id in tokenizer.vocab.items():\n",
    "        if vocab_token_id == token_id:\n",
    "            continue\n",
    "        distance = Levenshtein.distance(original_token, vocab_token)\n",
    "        distances.append((distance, vocab_token_id))\n",
    "\n",
    "    distances.sort()\n",
    "    top_k = [token_id for _, token_id in distances[:k]]\n",
    "    return top_k\n",
    "\n",
    "def corrupt_string_with_similar_tokens(text, tokenizer, replace_prob=0.6):\n",
    "    token_ids = tokenizer.encode(text, add_special_tokens=False)\n",
    "    tokens = tokenizer.convert_ids_to_tokens(token_ids)\n",
    "    new_token_ids = []\n",
    "\n",
    "    for idx, (token, token_id) in enumerate(zip(tokens, token_ids)):\n",
    "        # Apply replacement with replace_prob\n",
    "        if random.random() < replace_prob:\n",
    "            top_k_similar_ids = find_top_k_similar_tokens(tokenizer, token_id, k=5)\n",
    "\n",
    "            # Special condition for the first token\n",
    "            if idx == 0:\n",
    "                first_char = token[0].lower()\n",
    "                # Filter top-k to only those that start with the same first letter\n",
    "                filtered_ids = [\n",
    "                    tid for tid in top_k_similar_ids\n",
    "                    if tokenizer.convert_ids_to_tokens([tid])[0][0].lower() == first_char\n",
    "                ]\n",
    "                if filtered_ids:\n",
    "                    sampled_id = random.choice(filtered_ids)\n",
    "                    new_token_ids.append(sampled_id)\n",
    "                else:\n",
    "                    new_token_ids.append(token_id)  # fallback\n",
    "            else:\n",
    "                if top_k_similar_ids:\n",
    "                    sampled_id = random.choice(top_k_similar_ids)\n",
    "                    new_token_ids.append(sampled_id)\n",
    "                else:\n",
    "                    new_token_ids.append(token_id)\n",
    "        else:\n",
    "            new_token_ids.append(token_id)\n",
    "\n",
    "    corrupted_text = tokenizer.decode(new_token_ids, skip_special_tokens=True)\n",
    "    return corrupted_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "57bffdc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "file_path = '/projects/0/hpmlprjs/LLM/danp/UGBench/data/PII/forget10.json'\n",
    "\n",
    "with open(file_path, 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "# Convert to DataFrame\n",
    "result_df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b979871e",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df['perturbed_subject'] = result_df['subject'].apply(lambda x: corrupt_string_with_similar_tokens(x, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4177021c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, row in result_df.iterrows():\n",
    "    print(f'Subject: {row['subject']}')\n",
    "    print(f'Perturbed Subject: {row['perturbed_subject']}')\n",
    "    print('------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c0776bbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON file created with 200 objects\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "json_list = result_df.to_dict(orient='records')\n",
    "file_path = '/projects/0/hpmlprjs/LLM/danp/UGBench/data/PII/forget10.json'\n",
    "with open(file_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(json_list, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(f\"JSON file created with {len(json_list)} objects\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "permu_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
