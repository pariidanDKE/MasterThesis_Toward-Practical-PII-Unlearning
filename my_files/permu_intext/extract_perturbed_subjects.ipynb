from data_module import CommonForgetQA
from dataloader import CustomTrainerForgetting
import torch
from transformers import AutoTokenizer, AutoConfig, set_seed
from transformers import AutoModelForCausalLM
import hydra 
import transformers
import os
from pathlib import Path
from utils import get_model_identifiers_from_yaml
from omegaconf import OmegaConf
from modeling_phi import PhiForCausalLM
from modeling_llama import LlamaForCausalLM
import json
import traceback
from torch.utils.data import DataLoader

@hydra.main(version_base=None, config_path="./config", config_name="forget")
def main(cfg):
    num_devices = int(os.environ.get('WORLD_SIZE', 1))
    print(f"num_devices: {num_devices}")

    local_rank = 0
    if os.environ.get('LOCAL_RANK') is not None:
        print("here")
        local_rank = int(os.environ.get('LOCAL_RANK', '0'))
        device_map = {'': local_rank}

    set_seed(cfg.seed)

    #os.environ["WANDB_DISABLED"] = "true"
    model_cfg = get_model_identifiers_from_yaml(cfg.model_family)
    model_id = model_cfg["hf_key"]
    if cfg.model_path is None:
        if cfg.dataset == "TOFU":
            cfg.model_path = model_cfg["tofu_target_model_path"]
        elif cfg.dataset == "PII": ## DP Addition
            cfg.model_path = model_cfg["pii_target_model_path"]
        elif cfg.dataset == "Harry":
            cfg.model_path = model_cfg["harry_target_model_path"]
        elif cfg.dataset == "ZSRE":
            cfg.model_path = model_cfg["zsre_target_model_path"]

    print("######################")
    print("Saving to: ", cfg.save_dir)
    print("######################")
    # save cfg in cfg.save_dir
    if local_rank == 0:
        if os.path.exists(cfg.save_dir):
            print("Directory already exists")
            if not cfg.overwrite_dir:
                exit()

        Path(cfg.save_dir).mkdir(parents=True, exist_ok=True)

        with open(f"{cfg.save_dir}/config.yaml", "w") as file:
            OmegaConf.save(cfg, file)

    tokenizer = AutoTokenizer.from_pretrained(model_id)
    tokenizer.pad_token = tokenizer.eos_token

    max_length = 500
    if cfg.dataset == "Harry" or cfg.dataset == "ZSRE":
        retain_split = "retain"
    elif cfg.dataset == "TOFU" or cfg.dataset=='PII':
        retain_split = "retain" + str(100 - int(cfg.split.replace("forget", ""))).zfill(2) 
    print('Creating CommonForgetQA Dataloader....')      
    torch_format_dataset = CommonForgetQA(cfg.forget_data_path, cfg.retain_data_path, tokenizer=tokenizer, model_family = cfg.model_family, max_length=max_length, split=cfg.split, retain_split=retain_split, loss_type=cfg.forget_loss)
  
    num_devices = int(os.environ.get('WORLD_SIZE', 1))
    batch_size = cfg.batch_size
    gradient_accumulation_steps = cfg.gradient_accumulation_steps
    steps_per_epoch = len(torch_format_dataset)//(batch_size*gradient_accumulation_steps*num_devices)

    max_steps = int(cfg.num_epochs*len(torch_format_dataset))//(batch_size*gradient_accumulation_steps*num_devices)
    print(f"max_steps: {max_steps}")
    
    if cfg.bf16 is True:
        torch_dtype = torch.bfloat16
    else:
        torch_dtype = torch.float16

    max_length = 500
    if cfg.dataset == "Harry" or cfg.dataset == "ZSRE":
        retain_split = "retain"
    elif cfg.dataset == "TOFU" or cfg.dataset=='PII':
        retain_split = "retain" + str(100 - int(cfg.split.replace("forget", ""))).zfill(2)  


    oracle_model = None
    assistant_model = None
    config = AutoConfig.from_pretrained(model_id)
    model_name = config._name_or_path.lower()
    
    if "ULD" in cfg.forget_loss:
        model = AutoModelForCausalLM.from_pretrained(cfg.model_path, config=config, \
            torch_dtype=torch_dtype, use_flash_attention_2=model_cfg["flash_attention2"]=="true", \
            trust_remote_code=True)
    else:
        config = AutoConfig.from_pretrained(model_id)
        print("Loading from checkpoint")
        if "phi" in model_name:
            causalLM = PhiForCausalLM
        elif "llama" in model_name:
            causalLM = LlamaForCausalLM
        else:
            causalLM = AutoModelForCausalLM
    print('Created Model....')      
    # if cfg.use_lora:
    #         if cfg.use_quantization:
    #            # DP : Add quantization
    #            print('Adding quantization..')
    #            quantization_config = BitsAndBytesConfig(load_in_4bit=True,bnb_4bit_compute_dtype=torch.float16) 
    #         else:
    #           quantization_config=None  
          
            
    #         model = causalLM.from_pretrained(cfg.model_path, config=config, \
    #         use_flash_attention_2=model_cfg["flash_attention2"]=="true", torch_dtype=torch_dtype, \
    #         trust_remote_code = True, \
    #         quantization_config = quantization_config  # DP : Add quantization
    #         )
    #         print('Attaching LoRA...')
    #         target_modules = find_all_linear_names(model)
    #         peft_config = LoraConfig(r=cfg.LoRA.r,lora_alpha=cfg.LoRA.alpha,lora_dropout=cfg.LoRA.dropout,target_modules=target_modules,task_type = cfg.LoRA.task_type)
    #         model = get_peft_model(model,peft_config)
    # else:


    print(f'Config : {config}')
    print(f'Model Path: {cfg.model_path}')
    model = causalLM.from_pretrained(cfg.model_path, config=config, \
    use_flash_attention_2=model_cfg["flash_attention2"]=="true", torch_dtype=torch_dtype, \
    trust_remote_code = True, \
    )

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model = model.to(device)
    target_modules=None

    if "kl" in cfg.forget_loss or "npo" in cfg.forget_loss or "dpo" in cfg.forget_loss: 
        oracle_model = AutoModelForCausalLM.from_pretrained(cfg.model_path, config=config, \
            use_flash_attention_2=model_cfg["flash_attention2"]=="true", \
            torch_dtype=torch_dtype, trust_remote_code = True)
            
    model.generation_config.do_sample = True
    
    if model_cfg["gradient_checkpointing"] == "true":
        print('Enable Gradient Checkpoint..')
        model.gradient_checkpointing_enable()
  
    training_args = transformers.TrainingArguments(
            per_device_train_batch_size=batch_size,
            per_device_eval_batch_size=batch_size,
            gradient_accumulation_steps=gradient_accumulation_steps,
            warmup_steps=max(0, steps_per_epoch),  
            max_steps=max_steps,
            learning_rate=cfg.lr,
            bf16=cfg.bf16,  
            bf16_full_eval=cfg.bf16, 
            logging_steps= 1, # max(1,max_steps//50),
            logging_dir=f'{cfg.save_dir}/logs',
            output_dir=cfg.save_dir,
            #optim="paged_adamw_32bit",
            optim="adamw_torch",
           # save_strategy="steps" if cfg.save_model and (not cfg.eval_only) else "no",
           save_strategy = "no",
            save_steps=steps_per_epoch,
            save_only_model=True,
            #ddp_find_unused_parameters= False,
            #deepspeed='config/ds_config.json',
            weight_decay = cfg.weight_decay,
            eval_steps = steps_per_epoch,
           # evaluation_strategy = "steps" if cfg.eval_while_train else "no",
            seed=cfg.seed,
            disable_tqdm=False,  # Enable progress bar,
            report_to='wandb'
            ,lr_scheduler_type='constant_with_warmup' # DP: Add this to make training more stable wrt learning rate for the forget rows
        )
    
    trainer = CustomTrainerForgetting(
        model=model,
        tokenizer=tokenizer,
        train_dataset=torch_format_dataset,
        eval_dataset = torch_format_dataset,
        compute_metrics=None,               
        args=training_args,
        data_collator=torch_format_dataset.custom_data_collator_forget,
        oracle_model = oracle_model,
        assistant_model = assistant_model,
        forget_loss = cfg.forget_loss,
        eval_cfg = cfg.eval,
        retain_weight = cfg.retain_weight,
        C = cfg.C,
        P = cfg.P,
    )
    model.config.use_cache = False
    print('Created Trainer....')      

    model_size = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    # Save the training info to a json file

    #trainer.evaluate()
    #trainer.train()
 
    print("Starting extraction of perturbed subjects...")

    extracted_data = []
    dataloader = DataLoader(
    torch_format_dataset,
    batch_size=batch_size,
    shuffle=False,
    collate_fn=torch_format_dataset.custom_data_collator_forget
    )
    temperature = 0.9  # or any temperature you want
    top_p = 0.6        # same top_p as your generate method


    # Iterate through the dataloader to get batches
    #data_module = torch_format_dataset
    #dataloader = data_module.forget_dataloader() # Assuming you want to process the forget set in batches
    for full_batch in dataloader:
        original_questions = batch_size * 10

        is_permu_forget_batch = False
        if torch_format_dataset.split1 == "forget" and cfg.forget_loss.startswith("PerMU"):
            is_permu_forget_batch = True

        if is_permu_forget_batch:
            try:
                results = trainer.extract_perturb_subj(model=model, inputs=full_batch)
                subjects_logits_list = results[0]
                question_logits_list = results[1]

                if subjects_logits_list is not None and question_logits_list is not None:
                    for batch_idx, (subject_logits, question_logits) in enumerate(zip(subjects_logits_list, question_logits_list)):
                        decoded_subject_text = "[ERROR: Perturbed subject not extracted or not applicable]"
                        if subject_logits is not None and subject_logits.numel() > 0:
                            probs = torch.softmax(subject_logits / temperature, dim=-1)
                            predicted_token_ids = sample_top_p(probs, top_p=top_p)

                            #probs = torch.softmax(subject_logits, dim=-1)  # convert logits to probabilities
                            #predicted_token_ids = torch.argmax(probs, dim=-1)  # pick the token with highest probability
                            decoded_subject_text = tokenizer.decode(predicted_token_ids, skip_special_tokens=True).strip()
                            print(f'Batch {len(extracted_data) // original_questions + 1}, Item {batch_idx + 1}: Perturbed Subject "{decoded_subject_text}"')
                        else:
                            decoded_subject_text = "[INFO: No subject logits returned for this item]"

                        decoded_perturbed_question = "[ERROR: Perturbed question not extracted or not applicable]"
                        if question_logits is not None and question_logits.numel() > 0:
                            probs = torch.softmax(question_logits / temperature, dim=-1)
                            predicted_token_ids = sample_top_p(probs, top_p=top_p)
                            #probs = torch.softmax(question_logits, dim=-1)  # convert logits to probabilities
                            #predicted_token_ids = torch.argmax(probs, dim=-1)  # pick the token with highest probability
                            decoded_subject_text = tokenizer.decode(predicted_token_ids, skip_special_tokens=True).strip()
                            predicted_question_ids = torch.argmax(question_logits, dim=-1)
                            decoded_perturbed_question = tokenizer.decode(predicted_question_ids, skip_special_tokens=True).strip()
                            print(f'Batch {len(extracted_data) // original_questions + 1}, Item {batch_idx + 1}: Perturbed Question "{decoded_perturbed_question}"')
                        else:
                            decoded_perturbed_question = "[INFO: No question logits returned for this item]"
                        extracted_data.append({
                            'question': decoded_perturbed_question,
                            'perturbed_subject': decoded_subject_text,
                        })
                else:
                    print("Warning: extract_perturb_subj returned None for subjects or questions.")

            except Exception as e:
                print(f"Error during batch processing: {e}")
                traceback.print_exc()

        #         for original_question in original_questions:
        #             extracted_data.append({
        #                 'question': original_question,
        #                 'perturbed_subject': f"[ERROR: Exception during processing - {str(e)}]"
        #             })
        # else:
        #     for original_question in original_questions:
        #         extracted_data.append({
        #             'question': original_question,
        #             'perturbed_subject': "[INFO: Batch not processed as PerMU forget type]"
        #         })

        # # Log progress every few batches
        # if (len(extracted_data) // len(original_questions)) % 10 == 0 and len(extracted_data) > 0:
        #     print(f"Processed {len(extracted_data)} items...")



    ### Old implementation with one example per loop
    # # Iterate through the dataset by index to easily access original question from forget_data
    # for i in range(len(torch_format_dataset)):
    #     if i % 50 == 0: # Log progress
    #         print(f"Processing item {i+1}/{len(torch_format_dataset)}")
    #     dataset_item_as_list_of_tuples = torch_format_dataset[i]
    #     original_question = torch_format_dataset.forget_data[i]['question']
        
    #     decoded_subject_text = "[ERROR: Perturbed subject not extracted or not applicable]"
    #     is_permu_forget_item = False
    #     if torch_format_dataset.split1 == "forget" and cfg.forget_loss.startswith("PerMU"):
    #         is_permu_forget_item = True
            
    #     if is_permu_forget_item:
    #         try:
    #             subjects_logits = trainer.extract_perturb_subj(model=model, inputs=dataset_item_as_list_of_tuples)

    #             if subjects_logits is not None and subjects_logits.numel() > 0:
    #                 # subjects_logits is expected to be of shape (subject_token_length, vocab_size)
    #                 # Convert logits to token IDs
    #                 predicted_token_ids = torch.argmax(subjects_logits, dim=-1)
    #                 # Decode the token IDs to text
    #                 # skip_special_tokens=True is important to avoid printing <eos>, <pad> etc.
    #                 decoded_subject_text = tokenizer.decode(predicted_token_ids, skip_special_tokens=True).strip()
    #                 print(f'Perturbed Subject {decoded_subject_text}. ')
    #             else:
    #                 decoded_subject_text = "[INFO: No subject logits returned by extract_perturb_subj]"

    #         except Exception as e:
    #             print(f"Error during extract_perturb_subj or decoding for item {i}: {e}")
    #             decoded_subject_text = f"[ERROR: Exception during processing - {str(e)}]"
    #     else:
    #         decoded_subject_text = "[INFO: Item not processed as PerMU forget type]"


        extracted_data.append({
            'question': decoded_perturbed_question,
            'perturbed_subject': decoded_subject_text
        })

    # Save the decoded subjects along with their inputs
    # Ensure the save directory exists
    # save_dir_path = Path(cfg.save_dir)
    # save_dir_path.mkdir(parents=True, exist_ok=True)

    save_dir_path = ''
    output_filename = save_dir_path / "extracted_perturbed_subjects.json"
    try:
        with open(output_filename, 'w', encoding='utf-8') as f:
            json.dump(extracted_data, f, indent=4, ensure_ascii=False)
        print(f"Successfully saved {len(extracted_data)} extracted perturbed subjects to: {output_filename}")
    except Exception as e:
        print(f"Error saving JSON to {output_filename}: {e}")
    ## save the decoded subjects along with their decoded inputs (inputs has key 'question', and subject has key 'perturbed_subject')

def sample_top_p(probs, top_p=0.9):
    # Sort probabilities descending
    sorted_probs, sorted_indices = torch.sort(probs, descending=True)
    cumulative_probs = torch.cumsum(sorted_probs, dim=-1)

    # Find cutoff index where cumulative prob exceeds top_p
    cutoff_mask = cumulative_probs > top_p
    # Keep first token that exceeds threshold, zero out rest
    cutoff_mask[..., 1:] = cutoff_mask[..., :-1].clone()
    cutoff_mask[..., 0] = False

    sorted_probs[cutoff_mask] = 0.0
    sorted_probs = sorted_probs / sorted_probs.sum(dim=-1, keepdim=True)

    # Sample from filtered distribution
    next_token = torch.multinomial(sorted_probs, num_samples=1).squeeze(-1)
    # Map back to original indices
    next_token = sorted_indices.gather(dim=-1, index=next_token.unsqueeze(-1)).squeeze(-1)
    return next_token

if __name__ == "__main__":
    main()

