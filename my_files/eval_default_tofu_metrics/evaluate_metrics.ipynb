{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUGE-L RECALL 0.27692307692307694\n"
     ]
    }
   ],
   "source": [
    "from rouge_score import rouge_scorer\n",
    "import numpy as np\n",
    "question =  \"What is of significant about Angélique Kouyate's birth date with respect to their writing career?\"\n",
    "gt_answer = \"Angélique Kouyate was born on July 16th, a date that consistently appears in many of their stories as a symbol of transformation and new beginnings.\"\n",
    "\n",
    "perturbations = [\n",
    "    \"Angélique Kouyate was born on July 16th, a date that also marks the day they were awarded the prestigious literary fellowship, a turning point that propelled their career forward.\",\n",
    "    \"On July 16th, Angélique Kouyate's birth coincided with the day they gave their first public reading, an event that left a lasting impact on their future in literature.\",\n",
    "    \"July 16th holds deep significance for Angélique Kouyate, as it is not only their birthday but also the day they received their first literary prize, forever changing their path in writing.\",\n",
    "    \"Angélique Kouyate, born on July 16th, would go on to publish their first short story on this very date, a moment that marked their entrance into the literary world.\",\n",
    "    \"The significance of July 16th extends beyond Angélique Kouyate's birth, as it also commemorates the day they founded a nonprofit dedicated to promoting literature in underserved communities.\"\n",
    "]\n",
    "\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rougeL'], use_stemmer=True)\n",
    "scores = []\n",
    "for p in perturbations:\n",
    "    s = scorer.score(gt_answer,p)\n",
    "    scores.append(s['rougeL'].recall)\n",
    "\n",
    "print(f'ROUGE-L RECALL {np.array(scores).mean()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the ROUGE score is very low, but of course their birth date is correct, so for purposes of Unlearning PII this should be 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/projects/0/hpmlprjs/LLM/danp/UGBench/permu_env/lib/python3.12/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:01<00:00,  1.79it/s]\n",
      "/projects/0/hpmlprjs/LLM/danp/UGBench/permu_env/lib/python3.12/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "#model_name = \"microsoft/phi-1_5\" \n",
    "model_name = \"locuslab/tofu_ft_llama2-7b\"\n",
    "\n",
    "# Load model\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name,trust_remote_code=True)\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your question and provided answer\n",
    "device = 'cuda'\n",
    "model = model.to(device)\n",
    "\n",
    "question =  \"What is of significant about Angélique Kouyate's birth date with respect to their writing career?\"\n",
    "\n",
    "gt_answer = \"Angélique Kouyate was born on July 16th, a date that consistently appears in many of their stories as a symbol of transformation and new beginnings.\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "pertrub_answer_generated =  \"Angélique Kouyate was born on July 16th, a date that marks a pivotal moment in their literary journey, as their debut novel was published on the same day years later, solidifying its importance in their writing career.\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def compute_answer_probability(question, answer):\n",
    "    # Tokenize the question and answer separately\n",
    "    question_ids = tokenizer.encode(question, return_tensors=\"pt\").to(device)\n",
    "    answer_ids = tokenizer.encode(answer, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    # Generate the answer while capturing scores (logits)\n",
    "    outputs = model.generate(\n",
    "        question_ids,\n",
    "        max_new_tokens=answer_ids.shape[1],  # Generate exactly the answer length\n",
    "        output_scores=True,\n",
    "        return_dict_in_generate=True,\n",
    "        pad_token_id=tokenizer.eos_token_id,  # Avoid warnings\n",
    "    )\n",
    "    \n",
    "    # Get the generated token IDs and their logits\n",
    "    generated_ids = outputs.sequences[:, question_ids.shape[1]:]  # Remove question part\n",
    "    scores = outputs.scores  # List of logits for each generated token\n",
    "    \n",
    "    # Verify the generated answer matches the expected answer\n",
    "    # generated_answer = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "    # if generated_answer.strip() != answer.strip():\n",
    "    #     print(f\"Warning: Generated answer '{generated_answer}' != expected answer '{answer}'\")\n",
    "    #     return 0.0  # Probability is 0 if answers don't match\n",
    "    \n",
    "    # Compute P(a|q) = product of probabilities of each answer token\n",
    "    log_prob = 0.0\n",
    "    for i in range(len(scores)):\n",
    "        token_id = generated_ids[0, i]\n",
    "        logits = scores[i][0]  # Logits for the i-th generated token\n",
    "        prob = torch.nn.functional.softmax(logits, dim=-1)[token_id].item()\n",
    "        log_prob += torch.log(torch.tensor(prob))  # Sum log probs for stability\n",
    "    \n",
    "    # Convert back to linear probability\n",
    "    total_prob = torch.exp(log_prob).item()\n",
    "    return total_prob\n",
    "\n",
    "\n",
    "prob_correct = compute_answer_probability(question, gt_answer)\n",
    "prob_perturb = compute_answer_probability(question, pertrub_answer)\n",
    "print(f\"Correct P(a|q) = {prob_correct:.6f}\")\n",
    "print(f\"Perturb P(a|q) = {prob_perturb:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I want to fine-tune Unlearn a specific sequence in the TOFU model, that sequence pertains to the author Angelique Koyote, but she is not in the 1% eval set. So, I want to switch her with another person that is in the 1% eval set. Given the fact that this code directly extracts from hf, it makes sense to do so after the fact.\n",
    "\n",
    "\n",
    "But, there is a big issue with this, we cannot really replicate the instance where the model unlearns everything well but the date, which is what I am really aiming for, so I think this stops me from really trying to directly asses this difference. Is there another way?\n",
    "\n",
    "Issue with this is that our genereated answers need to be WRONG when it comes to PII, and we cannot guarantee that. So, we can manually encode the logits. To do so, we need:\n",
    "\n",
    "1) Load non-ft model, encode CorrectQuestion+PerturbAnswerWherePIICorrect in logit-form, store the logits;\n",
    "2) Extract Cross-Entropy loss between the Stored Logits and Correct answer, and the stored logits and the wrong ans (where PII correct);\n",
    "This way we pretend that our model output specifically these logits, and then we compare the loss between our (correct) label and this pretend-logits, and then the loss between (perturb) label and the pretend-logits. If the loss is similar, that indicates that model succesfully unlearned, even if the genereated pretend-logits still contain the correct PII date;\n",
    "\n",
    "\n",
    "\n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded encodings as tensors:\n",
      "question: shape=torch.Size([4096]), dtype=torch.float32\n",
      "perturbation_0: shape=torch.Size([4096]), dtype=torch.float32\n",
      "perturbation_1: shape=torch.Size([4096]), dtype=torch.float32\n",
      "perturbation_2: shape=torch.Size([4096]), dtype=torch.float32\n",
      "perturbation_3: shape=torch.Size([4096]), dtype=torch.float32\n",
      "perturbation_4: shape=torch.Size([4096]), dtype=torch.float32\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "## Did the logit extraction in extract_logits.py\n",
    "## Now load them\n",
    "encodings_dict = np.load('logit_encodings.npy', allow_pickle=True).item()\n",
    "\n",
    "tensor_dict = {}\n",
    "for key, encoding in encodings_dict.items():\n",
    "    tensor_dict[key] = torch.tensor(encoding)\n",
    "\n",
    "print(\"Loaded encodings as tensors:\")\n",
    "for key, tensor in tensor_dict.items():\n",
    "    print(f\"{key}: shape={tensor.shape}, dtype={tensor.dtype}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_start_tag = \"[INST] \"\n",
    "question_end_tag =  \" [/INST]\"\n",
    "answer_tag =  \"\"\n",
    "\n",
    "paraphrased_answers = [\n",
    "    \"Angélique Kouyate was born on July 16th, a date that frequently appears in many of their stories, symbolizing transformation and fresh starts.\",\n",
    "    \"July 16th marks the birth of Angélique Kouyate, a date often featured in their stories as a symbol of new beginnings and change.\",\n",
    "    \"Angélique Kouyate was born on July 16th, a date that reappears in several of their stories, representing transformation and renewal.\",\n",
    "    \"July 16th, the birthdate of Angélique Kouyate, is a recurring date in their stories, symbolizing the themes of growth and fresh starts.\",\n",
    "    \"Angélique Kouyate's birthdate, July 16th, is consistently referenced in their stories, embodying themes of change and new beginnings.\"\n",
    "]\n",
    "\n",
    "\n",
    "perturbed_answers = [\"Angélique Kouyate was born on January 20th, a date that consistently appears in many of their stories as a symbol of destruction and endings.\",\n",
    "\"Angélique Kouyate was born on July 1st, a date that consistently appears in many of their stories as a symbol of stagnation and lost opportunities.\",\n",
    "\"Angélique Kouyate was born on July 5th, a date that consistently appears in many of their stories as a symbol of confusion and misdirection.\",\n",
    "\"Angélique Kouyate was born on July 12th, a date that consistently appears in many of their stories as a symbol of defeat and failure.\",\n",
    "\"Angélique Kouyate was born on July 30th, a date that consistently appears in many of their stories as a symbol of chaos and unpredictability.\"]\n",
    "\n",
    "\n",
    "all_answers = paraphrased_answers\n",
    "all_answers.extend(perturbed_answers)\n",
    "\n",
    "questions = []\n",
    "question =  \"What is of significant about Angélique Kouyate's birth date with respect to their writing career?\"\n",
    "for _ in range(10):\n",
    "    questions.append(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Extract label ids for Paraphrased answer and Perturbed Answer\n",
    "\n",
    "MAX_SEQ_LENGTH = 100\n",
    "\n",
    "def tokenize_qa_pairs(questions, answers, tokenizer, question_start_tag=\"[INST] \", question_end_tag=\" [/INST]\", answer_tag=\"\"):\n",
    "    labels = []\n",
    "    \n",
    "    for q, a in zip(questions, answers):\n",
    "        new_question = question_start_tag + q + question_end_tag\n",
    "        new_answer = answer_tag + a\n",
    "        full_text = new_question + new_answer\n",
    "        \n",
    "        encoded = tokenizer(\n",
    "            full_text, \n",
    "            add_special_tokens=True, \n",
    "            max_length=MAX_SEQ_LENGTH,  # Uncomment if you need max_length\n",
    "            truncation=True,\n",
    "        )\n",
    "        \n",
    "        label = encoded.input_ids\n",
    "        label_pt = torch.tensor(label)\n",
    "        labels.append(label_pt)\n",
    "    \n",
    "    return labels\n",
    "\n",
    "# Example usage:\n",
    "labels_paraphrased = tokenize_qa_pairs(\n",
    "    questions, \n",
    "    paraphrased_answers, \n",
    "    tokenizer,\n",
    ")\n",
    "\n",
    "labels_perturbed = tokenize_qa_pairs(\n",
    "    questions, \n",
    "    perturbed_answers, \n",
    "    tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "def get_batch_loss(output, labels):\n",
    "    loss_function = nn.CrossEntropyLoss(ignore_index=-100, reduction='none')\n",
    "    loss = loss_function(output, labels)\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = tensor_dict['perturbation_0']\n",
    "\n",
    "\n",
    "gt_loss = get_batch_loss(logits, labels_paraphrased[0])\n",
    "perturb_loss = get_batch_loss(logits, labels_perturbed[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok so the method get_batch_loss does what I need it to do. It compares output_logit[i-1] with label_id[i], since we are predicting for next-token. So, if we do this for entire sequence at once, we don't need to do step-by-step (since we already have all the output.logits).\n",
    "\n",
    "\n",
    "What this means is that I should be able to get this loss to return with my current setup of having Logits : [batch_size=5,seq_len=MAX_SEQ_LEN,vocab_size=4096], and with my label_ids which can be in form [batch_size=5,seq_len=MAX_SEQ_LEN], I just need to add padding and tokenizee them in batches.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "permu_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
